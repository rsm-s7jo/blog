[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\nSunghoon Jo\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nSunghoon \nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\nYour Name\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Action: K-Means and KNN\n\n\n\n\nSunghoon Jo\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd Title\n\n\n\n\nSunghoon Jo\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\nSunghoon Jo\nJun 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe key experimental conditions included:\n- A standard appeal letter (control group)\n- A matching donation letter, where the organization promised to match donations at ratios of 1:1, 2:1, or 3:1\n- A challenge grant letter, which emphasized reaching a target donation amount before a match would be triggered\n\nThe goal was to examine whether donors are more likely to give (and give more) when their donations are matched — and whether the size of the match ratio (e.g., 3:1 vs. 1:1) affects this behavior.\nTheir findings contributed to the literature on charitable giving by showing that matching gifts significantly increase response rates, though larger match ratios do not always yield proportionally larger effects.\nThis project seeks to replicate and explore these results using the original dataset provided by the authors. It will assess the impact of match treatments on both the likelihood and size of donations, using statistical tests and visualizations to mirror the findings of the original 2007 study."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe key experimental conditions included:\n- A standard appeal letter (control group)\n- A matching donation letter, where the organization promised to match donations at ratios of 1:1, 2:1, or 3:1\n- A challenge grant letter, which emphasized reaching a target donation amount before a match would be triggered\n\nThe goal was to examine whether donors are more likely to give (and give more) when their donations are matched — and whether the size of the match ratio (e.g., 3:1 vs. 1:1) affects this behavior.\nTheir findings contributed to the literature on charitable giving by showing that matching gifts significantly increase response rates, though larger match ratios do not always yield proportionally larger effects.\nThis project seeks to replicate and explore these results using the original dataset provided by the authors. It will assess the impact of match treatments on both the likelihood and size of donations, using statistical tests and visualizations to mirror the findings of the original 2007 study."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nprint(df.shape)\nprint(df.describe())\n\n(50083, 51)\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\nprint(df['treatment'].value_counts())\nprint(df['control'].value_counts())\n\ntreatment\n1    33396\n0    16687\nName: count, dtype: int64\ncontrol\n0    33396\n1    16687\nName: count, dtype: int64\n\n\n\nprint(df.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nVariables Tested\nTo verify the success of the random assignment, I conducted balance checks on two pre-treatment variables:\n\nmrm2: months since last donation\nfreq: number of prior donations\n\nThese variables were tested using both t-tests and linear regression.\n\n\nmrm2\n\nfrom scipy import stats\n\n# NaN removal\ndf_clean = df[['treatment', 'control', 'mrm2']].dropna()\n\n# separate treatment and control groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# T-test \nt_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)\nprint(f\"T-test result: t={t_stat:.3f}, p={p_val:.3f}\")\n\nT-test result: t=0.120, p=0.905\n\n\n\n\nfreq\n\nimport statsmodels.api as sm\n\n# freq: number of prior donations\ndf_freq = df[['treatment', 'freq']].dropna()\nfreq_treat = df_freq[df_freq['treatment'] == 1]['freq']\nfreq_control = df_freq[df_freq['treatment'] == 0]['freq']\nt_stat_freq, p_val_freq = stats.ttest_ind(freq_treat, freq_control, equal_var=False)\nprint(f\"T-test (freq): t={t_stat_freq:.3f}, p={p_val_freq:.3f}\")\n\nT-test (freq): t=-0.111, p=0.912\n\n\n\n\nLinear Regression\n\nimport statsmodels.formula.api as smf\n\n# Regression\nresult = smf.ols('mrm2 ~ treatment', data=df_clean).fit()\nprint(result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Thu, 01 May 2025   Prob (F-statistic):              0.905\nTime:                        21:49:20   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Regression\ndf_freq['intercept'] = 1\nmodel_freq = sm.OLS(df_freq['freq'], df_freq[['intercept', 'treatment']])\nresult_freq = model_freq.fit()\nprint(result_freq.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Thu, 01 May 2025   Prob (F-statistic):              0.912\nTime:                        21:49:20   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nResult\n\n\n\n\n\n\n\n\n\n\nVariable\nT-test (p-value)\nRegression Coefficient\nRegression (p-value)\nBalanced?\n\n\n\n\nmrm2\n0.905\n0.0137\n0.905\nYes\n\n\nfreq\n0.912\n-0.0120\n0.912\nYes\n\n\n\n\n\nFor both mrm2 and freq, we fail to reject the null hypothesis that the means are equal between the treatment and control groups. Both the t-tests and linear regressions produce p-values well above the 0.05 threshold, and the treatment coefficients are near zero.\nThis strongly suggests that the random assignment was successful in producing comparable groups at baseline. There is no evidence that individuals in the treatment group differed systematically in these pre-treatment characteristics.\n\n\nWhy This Matters (Table 1 Context)\n\nTable 1 in Karlan & List (2007) presents these balance checks to demonstrate the integrity of the experimental design. By showing no significant differences between groups before treatment, the authors reinforce the internal validity of the study: any post-treatment differences in outcomes can reasonably be attributed to the treatment itself.\nThese balance tests provide strong evidence that randomization worked as intended. The treatment and control groups appear statistically equivalent on observed baseline characteristics, supporting causal inference in subsequent analyses."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n# charity donation ratio\ndonation_rates = df.groupby('treatment')['gave'].mean()\n\n# bar plot\ndonation_rates.plot(kind='bar')\nplt.xticks([0, 1], ['Control', 'Treatment'], rotation=0)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# NaN removal\ndf_clean = df[['treatment', 'gave']].dropna()\n\n# T-test\ngave_treat = df_clean[df_clean['treatment'] == 1]['gave']\ngave_control = df_clean[df_clean['treatment'] == 0]['gave']\nt_stat, p_val = stats.ttest_ind(gave_treat, gave_control, equal_var=False)\nprint(f\"T-test result: t = {t_stat:.3f}, p = {p_val:.3f}\")\n\n# linear regression: gave ~ treatment\ndf_clean['intercept'] = 1\nmodel = sm.OLS(df_clean['gave'], df_clean[['intercept', 'treatment']])\nresult = model.fit()\nprint(result.summary())\n\nT-test result: t = 3.209, p = 0.001\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Thu, 01 May 2025   Prob (F-statistic):            0.00193\nTime:                        21:49:20   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThere is a statistically significant difference in donation rates between groups. The treatment group had a higher likelihood of donating, and the p-value indicates this difference is unlikely to be due to chance.\nThe regression confirms the t-test: being in the treatment group increases the donation probability by 0.42 percentage points. Though small in magnitude, this effect is statistically significant, indicating that matched donations encourage giving.\n\nimport statsmodels.api as sm\n\n# Probit model\nprobit_model = sm.Probit(df_clean['gave'], df_clean[['intercept', 'treatment']])\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Thu, 01 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        21:49:20   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThe positive and significant coefficient from the probit regression supports the earlier findings: Being offered a matching donation increases the probability of giving. This matches Table 3, Column 1 in Karlan & List (2007), confirming the replication.\n\nHuman Behavior Insight\nThese results show that even a small external incentive — like a matching donation — can nudge people toward giving. It reinforces the idea that donors are influenced not just by personal motivation, but also by context and structure of giving opportunities.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# 1:1 vs 2:1\nt1 = df[df['ratio'] == 1]['gave'].dropna()\nt2 = df[df['ratio2'] == 1]['gave'].dropna()\nt_stat_1_2, p_val_1_2 = stats.ttest_ind(t1, t2, equal_var=False)\n\n# 2:1 vs 3:1\nt3 = df[df['ratio3'] == 1]['gave'].dropna()\nt_stat_2_3, p_val_2_3 = stats.ttest_ind(t2, t3, equal_var=False)\n\nprint(f\"1:1 vs 2:1 → t = {t_stat_1_2:.3f}, p = {p_val_1_2:.3f}\")\nprint(f\"2:1 vs 3:1 → t = {t_stat_2_3:.3f}, p = {p_val_2_3:.3f}\")\n\n1:1 vs 2:1 → t = -0.965, p = 0.335\n2:1 vs 3:1 → t = -0.050, p = 0.960\n\n\nThere is no statistically significant difference in donation rates between the 1:1 and 2:1 match groups, or between 2:1 and 3:1 groups. These results suggest that increasing the match ratio does not meaningfully affect whether people donate.\n\n# 1:1 dummy variable\ndf['ratio1'] = ((df['ratio'] == 1) & (df['ratio2'] != 1) & (df['ratio3'] != 1)).astype(int)\n\nmodel = sm.OLS(df['gave'], sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']].fillna(0)))\nresult = model.fit()\nprint(result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Thu, 01 May 2025   Prob (F-statistic):             0.0118\nTime:                        21:49:20   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nControlling for other match types, 2:1 and 3:1 match ratios are statistically significant predictors of giving — and lead to ~0.48–0.49 percentage point increases in donation rate compared to the baseline. However, the magnitude of the difference between match levels is small and likely not practically meaningful.\n\nrate_1_1 = df[df['ratio1'] == 1]['gave'].mean()\nrate_2_1 = df[df['ratio2'] == 1]['gave'].mean()\nrate_3_1 = df[df['ratio3'] == 1]['gave'].mean()\n\nprint(f\"2:1 - 1:1 = {rate_2_1 - rate_1_1:.4f}\")\nprint(f\"3:1 - 2:1 = {rate_3_1 - rate_2_1:.4f}\")\n\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\nWhile increasing the match ratio leads to slightly higher donation rates, the marginal gains from going beyond 1:1 are extremely small. This aligns with the paper’s “figures suggest” comment — indicating that larger matches might not be more effective in practice.\nThese results show that although higher match ratios appear to encourage slightly more giving, the differences are tiny and inconsistent. The data suggest that simply offering a match (vs. no match) matters, but offering bigger matches (e.g., 3:1 instead of 2:1) doesn’t have a clear benefit. People may be motivated by the presence of a match, but not necessarily its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport statsmodels.formula.api as smf\nresult_amt = smf.ols('amount ~ treatment', data=df).fit()\nprint(result_amt.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Thu, 01 May 2025   Prob (F-statistic):             0.0628\nTime:                        21:49:20   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nBeing in the treatment group is associated with an average donation increase of ~$0.15, but this result is not statistically significant at the 5% level (p ≈ 0.06). This suggests weak evidence that the matching offer may slightly increase giving amounts on average, but the effect is small and uncertain.\n\ndonors = df[df['gave'] == 1]\nresult_amt_donors = smf.ols('amount ~ treatment', data=donors).fit()\nprint(result_amt_donors.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Thu, 01 May 2025   Prob (F-statistic):              0.561\nTime:                        21:49:20   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAmong people who did give, being in the treatment group is associated with a slightly lower donation, but the difference is not statistically significant. This means the matching offer does not appear to increase the size of donations from those already motivated to give.\n\nimport matplotlib.pyplot as plt\n\ndonors_t = donors[donors['treatment'] == 1]\ndonors_c = donors[donors['treatment'] == 0]\n\nplt.hist(donors_c['amount'], bins=30, alpha=0.6)\nplt.axvline(donors_c['amount'].mean(), color='red', linestyle='dashed', label='Mean')\nplt.title(\"Donation Amounts - Control Group\")\nplt.legend()\nplt.show()\n\nplt.hist(donors_t['amount'], bins=30, alpha=0.6)\nplt.axvline(donors_t['amount'].mean(), color='red', linestyle='dashed', label='Mean')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth treatment and control groups have right-skewed donation distributions (long tail of large donations). The average donation is marked with a red dashed line on each plot. Visually, the distributions are very similar, supporting the regression results.\nThese results suggest that:\n\nOffering a match may make someone slightly more likely to give,\nBut it does not meaningfully affect how much they give.\n\nThis implies that match offers work more by encouraging action, not by increasing generosity among existing donors. In other words: people give because of the match — not more because of the match."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# seed\nnp.random.seed(42)\n\n# difference between 10_000 samples\nn_total = 100000\ncontrol = np.random.binomial(1, 0.018, n_total)\ntreatment = np.random.binomial(1, 0.022, 10000)\n\n# slicing\ncontrol = control[:10000]\n\n# difference from each sample\ndifferences = treatment - control\n\n# cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, 10001)\n\n# plot\nplt.plot(cumulative_avg, label='Cumulative Difference')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Avg Difference')\nplt.title('Law of Large Numbers in Action')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the donation rate difference converged toward the true difference of 0.004, confirming the Law of Large Numbers in practice.\n\n\nCentral Limit Theorem\n\nimport seaborn as sns\n\nsample_sizes = [50, 200, 500, 1000]\ntrue_diff = 0.004\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n    for _ in range(1000):\n        c = np.random.binomial(1, 0.018, n)\n        t = np.random.binomial(1, 0.022, n)\n        avg_diffs.append(np.mean(t) - np.mean(c))\n    \n    sns.histplot(avg_diffs, kde=True, ax=axes[i], bins=30)\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True diff = 0.004')\n    axes[i].set_title(f'Sample size: {n}')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs the sample size increased, the distribution of the mean difference became more tightly centered around the true value and more bell-shaped, illustrating the Central Limit Theorem. Zero moved from being near the center (at small n) to clearly in the tail (at larger n), showing the power of sample size in detecting treatment effects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sunghoon",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "blog/project1/hw2_questions.html",
    "href": "blog/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#data-load-and-overview",
    "href": "blog/project1/hw2_questions.html#data-load-and-overview",
    "title": "Poisson Regression Examples",
    "section": "Data Load and Overview",
    "text": "Data Load and Overview\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Blueprinty data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Histogram comparing patent counts\nsns.histplot(data=df, x='patents', hue='iscustomer', kde=True, element='step')\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Firm Count\")\nplt.show()\n\n\n\n\n\n\n\n\nWe see a clear shift in the distribution of patent counts between Blueprinty customers and non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare mean number of patents\ndf.groupby('iscustomer')['patents'].mean().round(2)\n\n# Boxplot of firm age\nsns.boxplot(data=df, x='iscustomer', y='age')\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Region comparison\npd.crosstab(df['region'], df['iscustomer'], normalize='index') \\\n    .round(2) * 100\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.0\n17.0\n\n\nNortheast\n45.0\n55.0\n\n\nNorthwest\n84.0\n16.0\n\n\nSouth\n82.0\n18.0\n\n\nSouthwest\n82.0\n18.0\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLog-Likelihood Function\nWe assume that the number of patents awarded to each firm over the last 5 years follows a Poisson distribution. The Poisson density is defined as:\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) for stability\n\ndef poisson_log_likelihood(lmbda, y):\n    # log-likelihood of Poisson for a single value or array\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\nAs expected, the log-likelihood is maximized when λ = Y.\n\nimport matplotlib.pyplot as plt\n\n# Example: one firm with 4 patents\ny_obs = 4\nlambdas = np.linspace(0.1, 10, 200)\nlogliks = [poisson_log_likelihood(lmbda, y_obs) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.title(\"Log-Likelihood of Poisson for Y = 4\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(x=y_obs, color='red', linestyle='--', label='Y = λ MLE')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimator for the Poisson Model\nTaking the derivative of the log-likelihood function and setting it to zero gives:\n\\[\n\\frac{d\\ell}{d\\lambda} = -1 + \\frac{Y}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = Y\n\\]\nThis makes intuitive sense, since the mean of a Poisson distribution is equal to its rate parameter λ.\nIf we observe multiple firms, and let \\(Y_1, Y_2, ..., Y_n\\) be the number of patents for each firm, the MLE becomes:\n\\[\n\\lambda_{\\text{MLE}} = \\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n\\]\nThis confirms that the sample mean is the MLE for the Poisson parameter λ.\n\nfrom scipy import optimize\n\nY = df['patents'].values\n\n# Negative log-likelihood (since most optimizers minimize)\ndef neg_loglik(lmbda):\n    return -poisson_log_likelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE estimate of λ: {lambda_mle:.3f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.3f}\")\n\nMLE estimate of λ: 3.685\nSample mean of Y: 3.685\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "href": "blog/project1/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\nWe now extend the simple Poisson model to a Poisson regression model. Let:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = e^{X_i'\\beta}\n\\]\nThis allows the expected count to depend on firm-level covariates, such as age, region, and Blueprinty customer status.\n\n\nLog-Likelihood Function\n\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define log-likelihood function\ndef poisson_log_likelihood(beta, y, X):\n    beta = np.atleast_1d(beta) \n    lin_pred = X @ beta  # Xβ\n    lam = np.exp(lin_pred)  # inverse link: exp\n    return np.sum(y * lin_pred - lam - gammaln(y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy import optimize\n# Ignore warnings\nnp.seterr(over='ignore', invalid='ignore')\n\n# Create design matrix X\ndf['age_squared'] = df['age'] ** 2\n\nX = df[['age', 'age_squared', 'iscustomer']].copy()\n\n# One-hot encode regions (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=df.index, name='intercept'), X, region_dummies], axis=1)\n\ny = df['patents'].values\nX_mat = X.values.astype(float) \n\n\n# Negative log-likelihood\ndef neg_loglik(beta):\n    return -poisson_log_likelihood(beta, y, X_mat)\n\n# Initial guess\nbeta_init = np.zeros(X_mat.shape[1])\n\n# Minimize\nresult = optimize.minimize(neg_loglik, beta_init, method='BFGS')\nbeta_hat = result.x\nhessian = result.hess_inv\n\n# Standard errors\nse_beta = np.sqrt(np.diag(hessian))\n\n# Summary table\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': se_beta\n}, index=X.columns)\n\ncoef_table.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n0.0\n1.0\n\n\nage\n0.0\n1.0\n\n\nage_squared\n0.0\n1.0\n\n\niscustomer\n0.0\n1.0\n\n\nNortheast\n0.0\n1.0\n\n\nNorthwest\n0.0\n1.0\n\n\nSouth\n0.0\n1.0\n\n\nSouthwest\n0.0\n1.0\n\n\n\n\n\n\n\nBased on the model, using Blueprinty is associated with an average increase of approximately X patents per firm (replace X with actual value). Because this is a log-linear model, we interpret the coefficients multiplicatively on the rate scale.\n\nInterpret Blueprinty Effect (Using Counterfactual Prediction)\n\n# Create X0 (all non-customers), X1 (all customers)\nX0 = X.copy()\nX0['iscustomer'] = 0\nX0 = X0[X.columns]\nX1 = X.copy()\nX1['iscustomer'] = 1\nX1 = X1[X.columns]\n\nX0_mat = X0.values.astype(float)\nX1_mat = X1.values.astype(float)\n\ny_pred_0 = np.exp(X0_mat @ beta_hat)\ny_pred_1 = np.exp(X1_mat @ beta_hat)\n\n# Average difference\neffect = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Estimated average effect of Blueprinty: {effect:.2f} more patents per firm\")\n\nEstimated average effect of Blueprinty: 0.00 more patents per firm"
  },
  {
    "objectID": "blog/project1/hw2_questions.html#airbnb-case-study",
    "href": "blog/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe use data on 40,000 Airbnb listings in NYC, scraped in March 2017.\nWe assume the number of reviews is a reasonable proxy for the number of bookings.\n\n\n\nData Load and Overview\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Airbnb data\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n# Histogram of number of reviews\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Listings\")\nplt.xlim(0, 100)  # Clip for visibility\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot: reviews by room type\nsns.boxplot(data=df, x='room_type', y='number_of_reviews')\nplt.title(\"Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Drop NA values in key columns\ndf_model = df[[\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]].dropna()\n\n\n\nPrepare Data for Modeling\n\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n# One-hot encode categorical vars\nX = df_model.copy()\nX['log_price'] = np.log1p(X['price'])  # stabilize price\nX['instant_bookable'] = (X['instant_bookable'] == 't').astype(int)\n\n# One-hot encode room_type\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\n\n# Define target and feature matrix\ny = X['number_of_reviews'].values\nX = X.drop(columns=['number_of_reviews'])\nX = pd.concat([pd.Series(1, index=X.index, name='intercept'), X], axis=1)\nX_mat = X.values.astype(float)\n\n\n\nEstimate Poisson Regression Model\n\nimport statsmodels.api as sm\n\nglm_poisson = sm.GLM(y, X_mat, family=sm.families.Poisson())\nglm_result = glm_poisson.fit()\n\nglm_result.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30148\n\n\nModel Family:\nPoisson\nDf Model:\n11\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2215e+05\n\n\nDate:\nThu, 01 May 2025\nDeviance:\n9.2283e+05\n\n\nTime:\n21:49:18\nPearson chi2:\n1.66e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.7237\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.3093\n0.026\n87.955\n0.000\n2.258\n2.361\n\n\nx1\n5.049e-05\n3.94e-07\n128.131\n0.000\n4.97e-05\n5.13e-05\n\n\nx2\n-0.1097\n0.004\n-28.467\n0.000\n-0.117\n-0.102\n\n\nx3\n0.0565\n0.002\n27.238\n0.000\n0.052\n0.061\n\n\nx4\n-0.0009\n2.54e-05\n-36.640\n0.000\n-0.001\n-0.001\n\n\nx5\n0.1073\n0.002\n71.426\n0.000\n0.104\n0.110\n\n\nx6\n-0.1015\n0.002\n-61.332\n0.000\n-0.105\n-0.098\n\n\nx7\n-0.0774\n0.002\n-42.335\n0.000\n-0.081\n-0.074\n\n\nx8\n0.3545\n0.003\n122.520\n0.000\n0.349\n0.360\n\n\nx9\n0.2999\n0.005\n58.052\n0.000\n0.290\n0.310\n\n\nx10\n0.1221\n0.003\n35.142\n0.000\n0.115\n0.129\n\n\nx11\n-0.0297\n0.009\n-3.208\n0.001\n-0.048\n-0.012\n\n\n\n\n\n\nInterpretation of Coefficients\nCoefficients in a Poisson model represent log changes in the expected count of reviews.\nFor example, a coefficient of 0.3 on room_type_Private room implies that private rooms are expected to receive ~35% more reviews than shared rooms, all else equal.\n\n\n\nPredicted Review Counts by Instant Bookability\n\nX0 = X.copy()\nX0['instant_bookable'] = 0\nX1 = X.copy()\nX1['instant_bookable'] = 1\n\nX0_mat = X0[X.columns].values.astype(float)\nX1_mat = X1[X.columns].values.astype(float)\n\ny_pred_0 = np.exp(X0_mat @ glm_result.params)\ny_pred_1 = np.exp(X1_mat @ glm_result.params)\n\nprint(\"Average predicted reviews:\")\nprint(f\"Not instant bookable: {y_pred_0.mean():.2f}\")\nprint(f\"Instant bookable: {y_pred_1.mean():.2f}\")\n\nAverage predicted reviews:\nNot instant bookable: 19.56\nInstant bookable: 27.88"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "blog/project1/index.html#sub-header",
    "href": "blog/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Resume"
  },
  {
    "objectID": "blog/project1/hw3_questions.html",
    "href": "blog/project1/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project1/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project1/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project1/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project1/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\n# Set seed for reproducibility\nnp.random.seed(123)\nrandom.seed(123)\n\n# Define attributes\nbrand = [\"N\", \"P\", \"H\"] # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\n# R's seq(8, 32, by=4) is a sequence from 8 to 32 incrementing by 4\nprice = np.arange(8, 33, 4) # 33 is exclusive, so it goes up to 32\n\n# Generate all possible profiles\n# Equivalent to R's expand.grid\nprofiles = pd.DataFrame(list(product(brand, ad, price)), columns=['brand', 'ad', 'price'])\nm = len(profiles) # Number of rows in profiles\n\n# Assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Number of respondents, choice tasks, and alternatives per task\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent’s data\ndef sim_one(id):\n    datlist = []\n\n    # Loop over choice tasks\n    for t in range(1, n_tasks + 1):\n        # Randomly sample 3 alternatives (better practice would be to use a design)\n        # Equivalent to R's profiles[sample(m, size=n_alts), ]\n        sampled_indices = random.sample(range(m), k=n_alts)\n        dat = profiles.iloc[sampled_indices].copy() # Use .copy() to avoid SettingWithCopyWarning\n        \n        # Add resp and task columns\n        dat['resp'] = id\n        dat['task'] = t\n        \n        # Compute deterministic portion of utility\n        # Equivalent to R's b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price)\n        dat['v'] = dat.apply(lambda row: b_util[row['brand']] + a_util[row['ad']] + p_util(row['price']), axis=1)\n        \n        # Add Gumbel noise (Type I extreme value)\n        # Equivalent to R's -log(-log(runif(n_alts)))\n        # Using inverse transform sampling for Gumbel distribution from uniform(0,1)\n        dat['e'] = -np.log(-np.log(np.random.rand(n_alts))) \n        dat['u'] = dat['v'] + dat['e']\n        \n        # Identify chosen alternative\n        # Equivalent to R's as.integer(dat$u == max(dat$u))\n        dat['choice'] = (dat['u'] == dat['u'].max()).astype(int)\n        \n        # Store task\n        datlist.append(dat)\n    \n    # Combine all tasks for one respondent\n    # Equivalent to R's do.call(rbind, datlist)\n    return pd.concat(datlist, ignore_index=True)\n\n# Simulate data for all respondents\n# Equivalent to R's do.call(rbind, lapply(1:n_peeps, sim_one))\nconjoint_data_list = [sim_one(i) for i in range(1, n_peeps + 1)]\nconjoint_data = pd.concat(conjoint_data_list, ignore_index=True)\n\n# Remove values unobservable to the researcher\n# Equivalent to R's conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\nconjoint_data = conjoint_data.loc[:, [\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Print results\nprint(\"Generated conjoint_data head:\")\nprint(conjoint_data.head())\nprint(\"\\nGenerated conjoint_data info:\")\nconjoint_data.info()\nprint(f\"\\nTotal number of respondents: {conjoint_data['resp'].nunique()}\")\nprint(f\"Total number of choice tasks (per respondent): {conjoint_data['task'].nunique()}\")\nprint(f\"Total number of alternatives (per task): {conjoint_data.groupby(['resp', 'task']).size().mean()}\")\n\nGenerated conjoint_data head:\n   resp  task brand   ad  price  choice\n0     1     1     N  Yes     20       1\n1     1     1     P  Yes     20       0\n2     1     1     N  Yes     28       0\n3     1     2     P   No     28       0\n4     1     2     P  Yes     20       1\n\nGenerated conjoint_data info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   resp    3000 non-null   int64 \n 1   task    3000 non-null   int64 \n 2   brand   3000 non-null   object\n 3   ad      3000 non-null   object\n 4   price   3000 non-null   int64 \n 5   choice  3000 non-null   int64 \ndtypes: int64(4), object(2)\nmemory usage: 140.8+ KB\n\nTotal number of respondents: 100\nTotal number of choice tasks (per respondent): 10\nTotal number of alternatives (per task): 3.0"
  },
  {
    "objectID": "blog/project1/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project1/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nconjoint_data = pd.read_csv('conjoint_data.csv', index_col=0)\n\n\n# Create dummy variables for categorical attributes\n# 'H' (Hulu) and 'No' (ad-free) are the reference categories in the utility function.\n# So, we will create dummies for 'N' (Netflix), 'P' (Prime), and 'Yes' (Ads).\n# pd.get_dummies will create columns like 'brand_N', 'brand_P', 'brand_H', 'ad_Yes', 'ad_No'.\n# We will then select and rename the specific columns that correspond to our beta parameters.\n\ndf_prep = conjoint_data.copy()\n\n# Convert 'brand' and 'ad' into dummy variables\n# We will create all dummies first, then select the ones corresponding to the non-reference categories\ndf_prep = pd.get_dummies(df_prep, columns=['brand', 'ad'], drop_first=False)\n\n# Rename the dummy columns to match the utility function's variable names\n# For 'brand': 'N' -&gt; 'Netflix', 'P' -&gt; 'Prime'\n# For 'ad': 'Yes' -&gt; 'Ads'\ndf_prep.rename(columns={\n    'brand_N': 'Netflix',\n    'brand_P': 'Prime',\n    'ad_Yes': 'Ads'\n}, inplace=True)\n\n# Drop the reference category dummy variables as their coefficients are implicitly zero\n# and they should not be included in the X matrix for direct estimation.\n# The original problem statement indicates 'Hulu' and 'Ad-free' as reference.\ndf_prep = df_prep.drop(columns=['brand_H', 'ad_No'])\n\n# Verify the prepared data structure\nprint(\"\\n--- Prepared df_prep head (with dummy variables for estimation) ---\")\nprint(df_prep.head())\nprint(\"\\n--- Prepared df_prep info ---\")\ndf_prep.info()\n\n# Final check: Ensure the columns for estimation are present\n# We need 'Netflix', 'Prime', 'Ads', 'price', 'resp', 'task', 'choice'\n# The columns 'resp' and 'task' are crucial for grouping alternatives within a choice set.\n\nprint(\"\\ndf_prep DataFrame prepared for estimation.\")\n\n\n--- Prepared df_prep head (with dummy variables for estimation) ---\n      task  choice  price  Netflix  Prime   Ads\nresp                                           \n1        1       1     28     True  False  True\n1        1       0     16    False  False  True\n1        1       0     16    False   True  True\n1        2       0     32     True  False  True\n1        2       1     16    False   True  True\n\n--- Prepared df_prep info ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 3000 entries, 1 to 100\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   task     3000 non-null   int64\n 1   choice   3000 non-null   int64\n 2   price    3000 non-null   int64\n 3   Netflix  3000 non-null   bool \n 4   Prime    3000 non-null   bool \n 5   Ads      3000 non-null   bool \ndtypes: bool(3), int64(3)\nmemory usage: 102.5 KB\n\ndf_prep DataFrame prepared for estimation."
  },
  {
    "objectID": "blog/project1/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project1/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nfrom scipy.optimize import minimize # Import only the minimize function\n\n# Define the log-likelihood function\n# beta_vector: [beta_netflix, beta_prime, beta_ads, beta_price]\ndef mnl_log_likelihood(beta_vector, data):\n    # IMPORTANT: Create a copy of the data to avoid modifying the original DataFrame\n    # in subsequent calls by the optimizer.\n    data = data.copy()\n\n    # Extract relevant columns for the X matrix. Their order must match beta_vector.\n    # Expected column names in 'data': 'Netflix', 'Prime', 'Ads', 'price'\n    X = data[['Netflix', 'Prime', 'Ads', 'price']].values\n\n    # --- CRITICAL FIX: Explicitly convert X to float64 type ---\n    # This ensures X is a proper float NumPy array for np.dot operation,\n    # preventing the 'float' object has no attribute 'exp' error.\n    X = X.astype(np.float64)\n    # --- End of fix ---\n\n    # Calculate deterministic utility V_ij = x_j'beta\n    V = np.dot(X, beta_vector)\n    \n    # Calculate exp(V) for all alternatives\n    # The previous error occurred because V was not a suitable NumPy array for np.exp.\n    # Ensuring numeric types in df_prep and explicitly converting X should fix this.\n    data['exp_V'] = np.exp(V)\n\n    # Group by 'resp' and 'task' to get the sum of exp(V) for each choice set.\n    # This sum will serve as the denominator for the probabilities.\n    sum_exp_V_per_task = data.groupby(['resp', 'task'])['exp_V'].transform('sum')\n\n    # Calculate probabilities P_i(j) = exp(V_ij) / sum(exp(V_ik))\n    # Add a small constant (1e-9) to the denominator for numerical stability, to prevent division by zero.\n    probabilities = data['exp_V'] / (sum_exp_V_per_task + 1e-9)\n\n    # Calculate log-likelihood: sum_i sum_j (delta_ij * log(P_i(j)))\n    # delta_ij is the 'choice' column (1 if chosen, 0 otherwise).\n    # Only terms where choice=1 will contribute to the sum.\n    log_probabilities = np.log(probabilities)\n    \n    # Sum the log-probabilities for the chosen alternatives.\n    total_log_likelihood = np.sum(np.where(data['choice'] == 1, log_probabilities, 0))\n\n    # We aim to maximize this likelihood, so for minimization, we return the negative log-likelihood.\n    return -total_log_likelihood\n\n# --- MLE Estimation ---\n\n# Initial guess for beta parameters\n# Order: [beta_netflix, beta_prime, beta_ads, beta_price]\ninitial_betas = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64) # Explicitly set dtype\n\n# Optimize the negative log-likelihood function\n# 'args' passes additional arguments (our data, i.e., df_prep) to the log-likelihood function.\nresult = minimize(mnl_log_likelihood, initial_betas, args=(df_prep,), method='BFGS') # BFGS is a good general-purpose optimizer.\n\n# Print optimization results\nprint(\"\\n--- MLE Optimization Results ---\")\nprint(result)\n\n# Extract MLEs\nmle_betas = result.x\nprint(\"\\nMLE Beta Parameters:\")\nprint(f\"Beta_Netflix: {mle_betas[0]:.4f}\")\nprint(f\"Beta_Prime:   {mle_betas[1]:.4f}\")\nprint(f\"Beta_Ads:     {mle_betas[2]:.4f}\")\nprint(f\"Beta_Price:   {mle_betas[3]:.4f}\")\n\n# --- Standard Errors and Confidence Intervals ---\n\n# The 'hess_inv' attribute from scipy.optimize.OptimizeResult is the inverse Hessian of the objective function\n# (which is the negative log-likelihood here). For the BFGS method, this directly provides the asymptotic covariance matrix of the MLEs.\ncov_matrix = result.hess_inv\n\n# Standard errors are the square root of the diagonal elements of the covariance matrix.\nstd_errors = np.sqrt(np.diag(cov_matrix))\n\n# Construct 95% Confidence Intervals\n# For large samples, MLEs are asymptotically normally distributed.\n# CI = Estimate +/- Z_alpha/2 * Standard Error\n# For a 95% CI, the Z-value (Z_alpha/2 for a two-tailed test) is approximately 1.96.\nz_value = 1.96\n\nconf_intervals = []\nfor i in range(len(mle_betas)):\n    lower_bound = mle_betas[i] - z_value * std_errors[i]\n    upper_bound = mle_betas[i] + z_value * std_errors[i]\n    conf_intervals.append((lower_bound, upper_bound))\n\n# Print results with standard errors and confidence intervals\nparam_names = ['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price']\nprint(\"\\n--- MLE Parameters with Standard Errors and 95% Confidence Intervals ---\")\nfor i, name in enumerate(param_names):\n    print(f\"{name}: {mle_betas[i]:.4f} (Std Err: {std_errors[i]:.4f}, 95% CI: [{conf_intervals[i][0]:.4f}, {conf_intervals[i][1]:.4f}])\")\n\n# Compare with true parameters (for mock data scenarios)\ntrue_betas = np.array([1.0, 0.5, -0.8, -0.1])\nprint(\"\\n--- True Parameters ---\")\nprint(f\"True Beta_Netflix: {true_betas[0]:.4f}\")\nprint(f\"True Beta_Prime:   {true_betas[1]:.4f}\")\nprint(f\"True Beta_Ads:     {true_betas[2]:.4f}\")\nprint(f\"True Beta_Price:   {true_betas[3]:.4f}\")\n\n\n--- MLE Optimization Results ---\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 879.8553701757969\n        x: [ 9.412e-01  5.016e-01 -7.320e-01 -9.948e-02]\n      nit: 14\n      jac: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n hess_inv: [[ 1.174e-02  4.981e-03 -2.176e-04 -1.126e-04]\n            [ 4.981e-03  8.934e-03  6.285e-04 -1.846e-05]\n            [-2.176e-04  6.285e-04  8.491e-03  1.119e-04]\n            [-1.126e-04 -1.846e-05  1.119e-04  4.064e-05]]\n     nfev: 110\n     njev: 22\n\nMLE Beta Parameters:\nBeta_Netflix: 0.9412\nBeta_Prime:   0.5016\nBeta_Ads:     -0.7320\nBeta_Price:   -0.0995\n\n--- MLE Parameters with Standard Errors and 95% Confidence Intervals ---\nBeta_Netflix: 0.9412 (Std Err: 0.1084, 95% CI: [0.7288, 1.1536])\nBeta_Prime: 0.5016 (Std Err: 0.0945, 95% CI: [0.3164, 0.6869])\nBeta_Ads: -0.7320 (Std Err: 0.0921, 95% CI: [-0.9126, -0.5514])\nBeta_Price: -0.0995 (Std Err: 0.0064, 95% CI: [-0.1120, -0.0870])\n\n--- True Parameters ---\nTrue Beta_Netflix: 1.0000\nTrue Beta_Prime:   0.5000\nTrue Beta_Ads:     -0.8000\nTrue Beta_Price:   -0.1000"
  },
  {
    "objectID": "blog/project1/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project1/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Ensure df_prep is defined (from Section 3) ---\n# This block is for self-contained execution. In a live notebook, you'd run Section 3 first.\ntry:\n    df_prep.head()\nexcept NameError:\n    print(\"df_prep DataFrame is not defined. Running mock data generation and preparation.\")\n    np.random.seed(123)\n    random.seed(123)\n    brand_options = [\"N\", \"P\", \"H\"]\n    ad_options = [\"Yes\", \"No\"]\n    price_options = np.arange(8, 33, 4)\n    n_peeps = 100\n    n_tasks = 10\n    n_alts = 3\n    mock_data_list = []\n    for resp_id in range(1, n_peeps + 1):\n        for task_id in range(1, n_tasks + 1):\n            sampled_profiles = pd.DataFrame(list(product(brand_options, ad_options, price_options)), columns=['brand', 'ad', 'price']).sample(n=n_alts, random_state=resp_id*task_id)\n            sampled_profiles['resp'] = resp_id\n            sampled_profiles['task'] = task_id\n            b_util_mock = {\"N\": 1.0, \"P\": 0.5, \"H\": 0}\n            a_util_mock = {\"Yes\": -0.8, \"No\": 0.0}\n            p_util_mock = lambda p: -0.1 * p\n            sampled_profiles['v_mock'] = sampled_profiles.apply(\n                lambda row: b_util_mock[row['brand']] + a_util_mock[row['ad']] + p_util_mock(row['price']), axis=1\n            )\n            sampled_profiles['e_mock'] = -np.log(-np.log(np.random.rand(n_alts)))\n            sampled_profiles['u_mock'] = sampled_profiles['v_mock'] + sampled_profiles['e_mock']\n            sampled_profiles['choice'] = (sampled_profiles['u_mock'] == sampled_profiles['u_mock'].max()).astype(int)\n            mock_data_list.append(sampled_profiles[['resp', 'task', 'brand', 'ad', 'price', 'choice']])\n    conjoint_data = pd.concat(mock_data_list, ignore_index=True)\n    \n    df_prep = conjoint_data.copy()\n    df_prep = pd.get_dummies(df_prep, columns=['brand', 'ad'], drop_first=False)\n    df_prep.rename(columns={\n        'brand_N': 'Netflix',\n        'brand_P': 'Prime',\n        'ad_Yes': 'Ads'\n    }, inplace=True)\n    df_prep = df_prep.drop(columns=['brand_H', 'ad_No'])\n    cols_to_ensure_numeric = ['Netflix', 'Prime', 'Ads', 'price', 'resp', 'task', 'choice']\n    for col in cols_to_ensure_numeric:\n        df_prep[col] = pd.to_numeric(df_prep[col], errors='coerce')\n    initial_rows_count = df_prep.shape[0]\n    df_prep.dropna(subset=cols_to_ensure_numeric, inplace=True)\n    if df_prep.shape[0] &lt; initial_rows_count:\n        print(f\"Warning: Dropped {initial_rows_count - df_prep.shape[0]} rows due to non-numeric values or NaNs during mock data prep.\")\n    print(\"df_prep DataFrame prepared from mock data.\")\n\n\n# Define the log-likelihood function (re-using from MLE section)\n# beta_vector: [beta_netflix, beta_prime, beta_ads, beta_price]\ndef mnl_log_likelihood(beta_vector, data):\n    data = data.copy() # Create a copy to avoid modifying the original DataFrame\n\n    X = data[['Netflix', 'Prime', 'Ads', 'price']].values\n    X = X.astype(np.float64) # Ensure X is float64 for np.dot\n\n    V = np.dot(X, beta_vector)\n    \n    # Clip V to prevent extreme values that cause inf/nan in exp(V)\n    V_clipped = np.clip(V, -500, 500) \n    \n    # Assign exp_V directly to a new column in the data DataFrame\n    data['exp_V'] = np.exp(V_clipped)\n\n    # Group by resp and task to get sum of exp(V) for each choice set\n    sum_exp_V_per_task = data.groupby(['resp', 'task'])['exp_V'].transform('sum')\n\n    # Add a small constant for numerical stability to prevent division by zero\n    probabilities = data['exp_V'] / (sum_exp_V_per_task + 1e-9)\n\n    # Add small constant to probabilities before log to avoid log(0) which is -inf\n    log_probabilities = np.log(probabilities + 1e-9) \n\n    total_log_likelihood = np.sum(np.where(data['choice'] == 1, log_probabilities, 0))\n\n    return total_log_likelihood\n\n# Define the log-prior function\ndef log_prior(beta_vector):\n    # Priors: N(0, 5) for beta_netflix, beta_prime, beta_ads\n    # N(0, 1) for beta_price\n    \n    # Log-PDF for Netflix, Prime, Ads (indices 0, 1, 2)\n    log_prior_binary = norm.logpdf(beta_vector[0], loc=0, scale=5) + \\\n                       norm.logpdf(beta_vector[1], loc=0, scale=5) + \\\n                       norm.logpdf(beta_vector[2], loc=0, scale=5)\n    \n    # Log-PDF for Price (index 3)\n    log_prior_price = norm.logpdf(beta_vector[3], loc=0, scale=1)\n    \n    return log_prior_binary + log_prior_price\n\n# Define the log-posterior function\ndef log_posterior(beta_vector, data):\n    # Check for invalid beta_vector values (e.g., if they are NaN or Inf)\n    if not np.all(np.isfinite(beta_vector)):\n        return -np.inf # Return negative infinity if parameters are invalid\n\n    # Calculate log-likelihood\n    log_lik = mnl_log_likelihood(beta_vector, data)\n    \n    # If log_lik is -inf or NaN (e.g., due to probabilities becoming 0 or numerical issues)\n    if not np.isfinite(log_lik):\n        return -np.inf\n\n    # Calculate log-prior\n    log_p = log_prior(beta_vector)\n    \n    # Sum log-likelihood and log-prior to get log-posterior\n    return log_lik + log_p\n\n# --- Metropolis-Hastings MCMC Sampler ---\n\n# Number of steps\nnum_steps = 11000\nburn_in = 1000\nretained_samples = 10000 # num_steps - burn_in\n\n# Initial parameters (can start from MLEs or random values)\ninitial_betas_mcmc = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64) \n\n# Proposal distribution standard deviations\nproposal_stds = np.array([np.sqrt(0.05), np.sqrt(0.05), np.sqrt(0.05), np.sqrt(0.005)], dtype=np.float64)\n\n# Storage for samples\nmcmc_samples = []\ncurrent_betas = initial_betas_mcmc.copy()\n\nprint(\"\\n--- Starting Metropolis-Hastings MCMC Sampling ---\")\nprint(f\"Total steps: {num_steps}, Burn-in: {burn_in}, Retained samples: {retained_samples}\")\n\nfor step in range(num_steps):\n    # Propose new parameters\n    proposed_betas = current_betas + np.random.normal(loc=0, scale=proposal_stds, size=len(current_betas))\n\n    # Calculate log-posterior for current and proposed parameters\n    log_post_current = log_posterior(current_betas, df_prep)\n    log_post_proposed = log_posterior(proposed_betas, df_prep)\n\n    # Calculate acceptance ratio in log-space\n    log_alpha = log_post_proposed - log_post_current\n\n    # Accept or reject the proposed parameters\n    if log_alpha &gt;= 0 or np.random.rand() &lt; np.exp(log_alpha):\n        current_betas = proposed_betas\n    \n    mcmc_samples.append(current_betas.copy()) # Store a copy of the current betas\n\n    if (step + 1) % 1000 == 0:\n        print(f\"Step {step + 1}/{num_steps} completed. Current betas: {current_betas}\")\n\nprint(\"\\n--- MCMC Sampling Complete ---\")\n\n# Discard burn-in samples and retain the rest\nposterior_samples_array = np.array(mcmc_samples[burn_in:])\nposterior_df = pd.DataFrame(posterior_samples_array, columns=['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price'])\n\n# --- Visualization: Trace Plot and Histogram ---\n\n# Choose one parameter to plot (e.g., Beta_Netflix)\nparam_to_plot = 'Beta_Netflix'\n\nplt.figure(figsize=(14, 6))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df[param_to_plot])\nplt.title(f'Trace Plot for {param_to_plot}')\nplt.xlabel('Sample Index')\nplt.ylabel('Parameter Value')\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# Histogram of posterior distribution\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df[param_to_plot], kde=True, bins=30)\nplt.title(f'Posterior Distribution of {param_to_plot}')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\n# --- Report Posterior Means, Standard Deviations, and 95% Credible Intervals ---\n\nprint(\"\\n--- Bayesian Posterior Summary Statistics ---\")\nposterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).loc[['mean', 'std', '2.5%', '97.5%']]\nprint(posterior_summary)\n\n# --- Comparison with Maximum Likelihood Approach ---\n# To ensure MLE results are always available for comparison, we define them here.\n# If you run the MLE section separately, these variables will be overwritten, which is fine.\n\nprint(\"\\n--- Calculating MLE results for comparison ---\")\ninitial_betas_mle = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64)\n# The mnl_log_likelihood function returns positive log-likelihood for MCMC,\n# but minimize needs negative log-likelihood for its objective.\nresult_mle = minimize(lambda b, d: -mnl_log_likelihood(b, d), initial_betas_mle, args=(df_prep,), method='BFGS')\nmle_betas = result_mle.x\ncov_matrix_mle = result_mle.hess_inv\nstd_errors_mle = np.sqrt(np.diag(cov_matrix_mle))\nz_value = 1.96\nconf_intervals_mle = []\nfor i in range(len(mle_betas)):\n    lower_bound = mle_betas[i] - z_value * std_errors_mle[i]\n    upper_bound = mle_betas[i] + z_value * std_errors_mle[i]\n    conf_intervals_mle.append((lower_bound, upper_bound))\nparam_names = ['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price']\nprint(\"MLE results obtained for comparison.\")\n\n\nprint(\"\\n--- Comparison: Bayesian MCMC vs. Maximum Likelihood Estimation ---\")\nprint(\"Parameter | Bayesian Mean | Bayesian Std | Bayesian 95% CI | MLE Estimate | MLE Std Err | MLE 95% CI\")\nprint(\"-----------------------------------------------------------------------------------------------------------------\")\nfor i, name in enumerate(param_names):\n    bayesian_mean = posterior_summary.loc['mean', name]\n    bayesian_std = posterior_summary.loc['std', name]\n    bayesian_ci_lower = posterior_summary.loc['2.5%', name]\n    bayesian_ci_upper = posterior_summary.loc['97.5%', name]\n    \n    mle_estimate = mle_betas[i]\n    mle_std_err = std_errors_mle[i]\n    mle_ci_lower = conf_intervals_mle[i][0]\n    mle_ci_upper = conf_intervals_mle[i][1]\n\n    print(f\"{name:&lt;9} | {bayesian_mean:13.4f} | {bayesian_std:12.4f} | [{bayesian_ci_lower:7.4f}, {bayesian_ci_upper:7.4f}] | {mle_estimate:12.4f} | {mle_std_err:11.4f} | [{mle_ci_lower:7.4f}, {mle_ci_upper:7.4f}]\")\n\nprint(\"\\n--- Interpretation ---\")\nprint(\"In general, for sufficiently large sample sizes, Bayesian posterior means and standard deviations (and credible intervals) are expected to be similar to MLE estimates and their standard errors (and confidence intervals).\")\nprint(\"Any notable differences could be due to:\")\nprint(\"1. The choice of prior distributions (especially if the prior is very informative).\")\nprint(\"2. Insufficient MCMC chain length or burn-in (check trace plots for convergence).\")\nprint(\"3. Differences in how standard errors are calculated (asymptotic vs. posterior sample).\")\nprint(\"4. Numerical stability issues in the likelihood function for extreme parameter values.\")\nprint(\"The results from both methods should provide insights into the preference weights for different streaming service attributes.\")\n\n\n--- Starting Metropolis-Hastings MCMC Sampling ---\nTotal steps: 11000, Burn-in: 1000, Retained samples: 10000\nStep 1000/11000 completed. Current betas: [ 1.10066399  0.63248538 -0.64393751 -0.09794073]\nStep 2000/11000 completed. Current betas: [ 0.74248622  0.66354872 -0.7527276  -0.10752908]\nStep 3000/11000 completed. Current betas: [ 1.0115884   0.49204654 -0.73226032 -0.09578803]\nStep 4000/11000 completed. Current betas: [ 0.85711719  0.35929496 -0.8045634  -0.10478445]\nStep 5000/11000 completed. Current betas: [ 1.03465998  0.69459888 -0.84064235 -0.09974663]\nStep 6000/11000 completed. Current betas: [ 0.88620057  0.51872741 -0.74652068 -0.09219769]\nStep 7000/11000 completed. Current betas: [ 1.05791435  0.56012718 -0.56787254 -0.0919589 ]\nStep 8000/11000 completed. Current betas: [ 1.08115541  0.58585425 -0.77121125 -0.09913206]\nStep 9000/11000 completed. Current betas: [ 0.85241723  0.2998144  -0.70238674 -0.10454084]\nStep 10000/11000 completed. Current betas: [ 0.94638304  0.48775865 -0.74193055 -0.09695354]\nStep 11000/11000 completed. Current betas: [ 0.84594064  0.56758692 -0.84981676 -0.1028459 ]\n\n--- MCMC Sampling Complete ---\n\n\n\n\n\n\n\n\n\n\n--- Bayesian Posterior Summary Statistics ---\n       Beta_Netflix  Beta_Prime  Beta_Ads  Beta_Price\nmean       0.946296    0.500675 -0.741247   -0.100196\nstd        0.114058    0.117083  0.078775    0.006058\n2.5%       0.747114    0.290832 -0.885374   -0.112216\n97.5%      1.218240    0.703372 -0.567873   -0.090597\n\n--- Calculating MLE results for comparison ---\nMLE results obtained for comparison.\n\n--- Comparison: Bayesian MCMC vs. Maximum Likelihood Estimation ---\nParameter | Bayesian Mean | Bayesian Std | Bayesian 95% CI | MLE Estimate | MLE Std Err | MLE 95% CI\n-----------------------------------------------------------------------------------------------------------------\nBeta_Netflix |        0.9463 |       0.1141 | [ 0.7471,  1.2182] |       0.9412 |      0.0917 | [ 0.7614,  1.1210]\nBeta_Prime |        0.5007 |       0.1171 | [ 0.2908,  0.7034] |       0.5016 |      0.0614 | [ 0.3812,  0.6220]\nBeta_Ads  |       -0.7412 |       0.0788 | [-0.8854, -0.5679] |      -0.7320 |      0.1230 | [-0.9731, -0.4909]\nBeta_Price |       -0.1002 |       0.0061 | [-0.1122, -0.0906] |      -0.0995 |      0.0070 | [-0.1131, -0.0858]\n\n--- Interpretation ---\nIn general, for sufficiently large sample sizes, Bayesian posterior means and standard deviations (and credible intervals) are expected to be similar to MLE estimates and their standard errors (and confidence intervals).\nAny notable differences could be due to:\n1. The choice of prior distributions (especially if the prior is very informative).\n2. Insufficient MCMC chain length or burn-in (check trace plots for convergence).\n3. Differences in how standard errors are calculated (asymptotic vs. posterior sample).\n4. Numerical stability issues in the likelihood function for extreme parameter values.\nThe results from both methods should provide insights into the preference weights for different streaming service attributes."
  },
  {
    "objectID": "blog/project1/hw3_questions.html#discussion",
    "href": "blog/project1/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nObservations about Parameter Estimates If we assume the data was not simulated (i.e., it represents real-world conjoint data) and we observe the parameter estimates:\nGeneral Observations on Estimates: You would typically examine the sign, magnitude, and statistical significance (or credibility) of each parameter.\nSigns: Positive coefficients indicate a preference for that attribute level (e.g., higher utility), while negative coefficients suggest an aversion (lower utility). Magnitude: The absolute magnitude of the coefficient reveals the strength of the preference or aversion. Larger absolute values imply a stronger impact on utility. Significance/Credibility: If the 95% confidence interval (for MLE) or credible interval (for Bayesian) does not include zero, it suggests that the attribute level has a statistically significant or credible impact on utility, indicating the true effect is unlikely to be zero. What does \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) mean? This implies that, holding all other factors constant (such as price and ad exposure), consumers derive higher utility from Netflix compared to Prime Video. Since Hulu (or the brand_H dummy) was typically set as the reference category with a coefficient of zero, this observation also suggests that consumers prefer Netflix more than Prime Video, and both Netflix and Prime Video are preferred over Hulu (assuming both \\(\\beta_{\\text{Netflix}}\\) and \\(\\beta_{\\text{Prime}}\\) are positive and greater than zero). The difference in their magnitudes (\\(\\beta_{\\text{Netflix}} - \\beta_{\\text{Prime}}\\)) represents the incremental utility gain from choosing Netflix over Prime Video. For example, if \\(\\beta_{\\text{Netflix}} = 1.0\\) and \\(\\beta_{\\text{Prime}} = 0.5\\), opting for Netflix instead of Prime provides an additional 0.5 units of utility.\nDoes it make sense that \\(\\beta_{\\text{price}}\\) is negative? Yes, it makes perfect sense. Price represents a cost to the consumer. A negative coefficient for price indicates that as the price of an alternative increases, the utility derived from that alternative decreases. This aligns with fundamental economic principles of consumer behavior: all else being equal, consumers prefer lower prices. A larger absolute value for \\(\\beta_{\\text{price}}\\) would imply that consumers are more sensitive to price changes.\nSimulating Data and Estimating Parameters for a Multi-Level (Hierarchical) Model A multi-level (also known as random-parameter or hierarchical) model is crucial for analyzing “real-world” conjoint data because it directly accounts for heterogeneity in preferences across individuals. Instead of assuming all respondents share the exact same underlying utility parameters (as in the simple MNL model you’ve implemented), it allows individual-specific parameters to vary according to a distribution (e.g., a normal distribution) across the population.\nHere’s a high-level overview of the changes needed to simulate data from and estimate parameters of such a model:\n\nSimulating Data for a Multi-Level Model: Individual-Specific Betas: Instead of a single “true” beta_vector for the entire population, you’d first define a population-level mean vector (\\(\\mu_{\\beta}\\)) and a covariance matrix (\\(\\Sigma_{\\beta}\\)) for the betas. Sampling Individual Betas: For each simulated respondent (\\(i\\)), you would then draw their individual beta_vector_i from a multivariate normal distribution: \\[\\beta_i \\sim \\mathcal{N}(\\mu_{\\beta}, \\Sigma_{\\beta})\\] Generating Choices: For each respondent i and each choice task t, you’d use their specific beta_vector_i to calculate the utilities (\\(V_{ijt} = x_{jt}'\\beta_i\\)) for all alternatives j in that task. Add extreme value (Gumbel) distributed error terms (\\(\\varepsilon_{ijt}\\)) and determine the chosen alternative (choice_ijt) based on the highest total utility (\\(U_{ijt} = V_{ijt} + \\varepsilon_{ijt}\\)), similar to your current simulation but personalized by individual betas. Data Structure: The simulated data would maintain a similar observation-level structure (resp, task, attributes, choice), but the underlying beta_vector used to generate each choice would vary by resp.\nEstimating Parameters for a Multi-Level Model: Estimating these models is significantly more complex than standard MLE or basic MCMC, often requiring advanced Bayesian methods.\n\nModel Formulation:\nLikelihood: The likelihood for an individual’s choices, given their specific betas, would remain similar to the MNL likelihood. Priors: You’d need to define priors not only for the individual betas (which are now treated as random variables drawn from a distribution) but also for the hyper-parameters of that distribution—specifically, the mean (\\(\\mu_{\\beta}\\)) and the covariance matrix (\\(\\Sigma_{\\beta}\\)) of the population-level beta distribution. Hierarchical Structure: The model explicitly links individual-level parameters to population-level parameters, allowing for “borrowing strength” across individuals while acknowledging individual differences. MCMC Sampler Changes:\nIncreased Parameter Space: The MCMC algorithm would need to sample not just the mean beta parameters but also the elements of the covariance matrix (\\(\\Sigma_{\\beta}\\)). For instance, with 4 betas, \\(\\Sigma_{\\beta}\\) is a \\(4 \\times 4\\) matrix, introducing many more parameters (4 variances and 6 covariances) to estimate. More Complex Proposal Distribution: The proposal distribution for sampling the individual beta_i’s and the elements of \\(\\Sigma_{\\beta}\\) would be more intricate. Specialized MCMC algorithms like Gibbs sampling or more advanced Metropolis-Hastings variants are commonly used, often drawing on techniques like slice sampling or Hamiltonian Monte Carlo (HMC) for greater efficiency, especially when dealing with the covariance matrix. Data Partitioning: The log-likelihood calculation would involve summing log-likelihoods over each individual’s choices, given their currently sampled individual-level betas. Convergence Challenges: Multi-level MCMC models often require longer burn-in periods, more samples, and meticulous tuning of proposal distributions to ensure proper convergence due to the higher dimensionality and interdependencies of parameters. Specialized Software: While possible to code from scratch (as you’ve done with basic MCMC), practitioners typically rely on robust probabilistic programming languages and libraries built for hierarchical modeling, such as: PyMC (Python) Stan (accessible via pystan in Python or rstan in R) JAGS/BUGS (older, but still used)\nIn essence, building and estimating a multi-level conjoint model involves a significant leap in complexity, moving from a single set of population parameters to a distribution of individual parameters characterized by population-level means and covariances."
  },
  {
    "objectID": "blog/project1/hw4_questions.html",
    "href": "blog/project1/hw4_questions.html",
    "title": "Machine Learning in Action: K-Means and KNN",
    "section": "",
    "text": "In this section, we apply the K-Means clustering algorithm to the Palmer Penguins dataset in order to explore natural groupings in the data without using species labels. K-Means is an unsupervised learning technique that partitions data into 𝑘 distinct clusters based on feature similarity.\nWe focus on two features — bill length and flipper length — and use both a custom implementation and scikit-learn’s built-in tools to perform clustering. To evaluate the quality of the clusters, we visualize the clustering process with an animation and apply quantitative metrics such as the Within-Cluster Sum of Squares (WCSS) and Silhouette Score to help determine the optimal number of clusters.\nThis analysis demonstrates how K-Means identifies patterns in biological measurements and how different values of 𝑘 affect the results.\n\n\nWe start by loading the Palmer Penguins dataset, a popular alternative to the Iris dataset, and selecting two continuous numerical features:\n\nbill_length_mm\nflipper_length_mm\n\nThese features are cleaned by removing rows with missing values (NaN). Then, we apply standard scaling using StandardScaler to normalize the feature values. This step is important because K-Means is sensitive to the scale of the features.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Select relevant features and drop NA\nX = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nWe implemented the K-Means algorithm from scratch using NumPy. Here’s how it works:\n\nInitialization: Randomly select k data points as initial cluster centroids.\nAssignment Step: Assign each point to the nearest centroid based on Euclidean distance.\nUpdate Step: Recalculate the centroids as the mean of all points assigned to each cluster.\nConvergence: Repeat steps 2–3 until centroids stabilize or the maximum number of iterations is reached.\n\nThis function returns the final cluster labels and centroids.\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(42)\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    for i in range(max_iters):\n        # Assign clusters\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        # Recalculate centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        # Convergence check\n        if np.allclose(centroids, new_centroids, atol=tol):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n\n\n\nTo visualize the clustering process over iterations, we created an animation using matplotlib.animation. The animation shows how centroids shift and how points get reassigned over 10 iterations. Points are colored by their current cluster label. Centroids are marked with black x symbols.\n\nimport matplotlib.animation as animation\ndef animate_kmeans(X, k, filename='kmeans_converged.gif', tol=1e-4, max_iters=100):\n    centroids = X[np.random.choice(len(X), k, replace=False)]\n    history = []\n\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        new_centroids = []\n        for j in range(k):\n            cluster_points = X[labels == j]\n            if len(cluster_points) == 0:\n                new_centroids.append(X[np.random.choice(len(X))])\n            else:\n                new_centroids.append(cluster_points.mean(axis=0))\n        new_centroids = np.array(new_centroids)\n\n        history.append((labels.copy(), centroids.copy()))\n\n        # Check for convergence\n        max_movement = np.max(np.linalg.norm(new_centroids - centroids, axis=1))\n        if max_movement &lt; tol:\n            break\n\n        centroids = new_centroids\n\n    # Ensure we have enough frames for the animation\n    for _ in range(5):\n        history.append((labels.copy(), centroids.copy()))\n\n    # animation\n    fig, ax = plt.subplots()\n\n    def update(frame):\n        ax.clear()\n        labels, centroids = history[frame]\n        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', s=40, alpha=0.6)\n        ax.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100)\n        ax.set_title(f'Iteration {frame+1}')\n        ax.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n        ax.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n\n    ani = animation.FuncAnimation(fig, update, frames=len(history), interval=800)\n    ani.save(filename, writer='pillow')\n    plt.close()\nanimate_kmeans(X_scaled, k=3, filename='kmeans_converged.gif')\n\n\n\n\n\n\nK-means animation\n\n\n\n\n\n\nTo determine the optimal number of clusters k, we applied two common metrics:\n\nWCSS (Within-Cluster Sum of Squares): Measures compactness of the clusters.\n\n\nLower values indicate tighter clusters.\nWe look for an “elbow point” in the WCSS plot where the rate of improvement slows down.\n\n\nSilhouette Score: Measures how similar each point is to its own cluster compared to other clusters.\n\n\nRanges from -1 to 1.\nHigher values indicate better-defined clusters.\n\nWe computed both metrics for 𝑘 = 2 to 7\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nK = range(2, 8)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X_scaled)\n    wcss.append(km.inertia_)  # Within-cluster sum of squares\n    silhouette_scores.append(silhouette_score(X_scaled, labels))\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K, wcss, marker='o')\nax[0].set_title('WCSS vs K')\nax[0].set_xlabel('Number of clusters')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K, silhouette_scores, marker='o')\nax[1].set_title('Silhouette Score vs K')\nax[1].set_xlabel('Number of clusters')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw4_questions.html#a.-k-means",
    "href": "blog/project1/hw4_questions.html#a.-k-means",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "We start by loading the Palmer Penguins dataset, a popular alternative to the Iris dataset, and selecting two continuous numerical features:\n\nbill_length_mm\nflipper_length_mm\n\nThese features are cleaned by removing rows with missing values (NaN). Then, we apply standard scaling using StandardScaler to normalize the feature values. This step is important because K-Means is sensitive to the scale of the features.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Select relevant features and drop NA\nX = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nWe implemented the K-Means algorithm from scratch using NumPy. Here’s how it works:\n\nInitialization: Randomly select k data points as initial cluster centroids.\nAssignment Step: Assign each point to the nearest centroid based on Euclidean distance.\nUpdate Step: Recalculate the centroids as the mean of all points assigned to each cluster.\nConvergence: Repeat steps 2–3 until centroids stabilize or the maximum number of iterations is reached.\n\nThis function returns the final cluster labels and centroids.\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(42)\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    for i in range(max_iters):\n        # Assign clusters\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        # Recalculate centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        # Convergence check\n        if np.allclose(centroids, new_centroids, atol=tol):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n\n\n\nTo visualize the clustering process over iterations, we created an animation using matplotlib.animation. The animation shows how centroids shift and how points get reassigned over 10 iterations. Points are colored by their current cluster label. Centroids are marked with black x symbols.\n\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\ndef animate_kmeans(X, k):\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    ims = []\n    for _ in range(10):\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', alpha=0.6)\n        centers = ax.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x')\n        ims.append([scatter, centers])\n        centroids = new_centroids\n    ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=1000)\n    ani.save('kmeans_animation.gif', writer='pillow')\n    plt.close()\n\n\nanimate_kmeans(X_scaled, k=3)\n\n\n\n\n\n\nK-means animation\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nK = range(2, 8)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X_scaled)\n    wcss.append(km.inertia_)  # Within-cluster sum of squares\n    silhouette_scores.append(silhouette_score(X_scaled, labels))\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K, wcss, marker='o')\nax[0].set_title('WCSS vs K')\nax[0].set_xlabel('Number of clusters')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K, silhouette_scores, marker='o')\nax[1].set_title('Silhouette Score vs K')\nax[1].set_xlabel('Number of clusters')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTo determine the optimal number of clusters k, we applied two common metrics:\n\nWCSS (Within-Cluster Sum of Squares): Measures compactness of the clusters.\n\n\nLower values indicate tighter clusters.\nWe look for an “elbow point” in the WCSS plot where the rate of improvement slows down.\n\n\nSilhouette Score: Measures how similar each point is to its own cluster compared to other clusters.\n\n\nRanges from -1 to 1.\nHigher values indicate better-defined clusters.\n\nWe computed both metrics for 𝑘 = 2 to 7"
  },
  {
    "objectID": "blog/project1/hw4_questions.html#b.-latent-class-mnl",
    "href": "blog/project1/hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/project1/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/project1/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "blog/project1/hw4_questions.html#b.-key-drivers-analysis",
    "href": "blog/project1/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "blog/project1/hw4_questions.html#k-means-clustering-unsupervised-learning",
    "href": "blog/project1/hw4_questions.html#k-means-clustering-unsupervised-learning",
    "title": "Add Title",
    "section": "1. K-Means Clustering (Unsupervised Learning)",
    "text": "1. K-Means Clustering (Unsupervised Learning)\n\nOverview\nIn this section, I implement the K-Means clustering algorithm from scratch and apply it to the palmer_penguins.csv dataset, using bill length and flipper length as the two features for clustering. I then compare the results to those generated by the built-in KMeans function from scikit-learn.\nTo evaluate the quality of clustering for various values of K (number of clusters), I compute and visualize both the within-cluster sum of squares (WCSS) and the silhouette score. Additionally, I visualize the step-by-step operation of the algorithm using an animated plot.\n\n\n\nDataset Description\nThe palmer_penguins dataset contains measurements for three penguin species observed in Antarctica: Adelie, Chinstrap, and Gentoo. For this analysis, I focus on two numerical features:\n\nbill_length_mm: length of the penguin’s bill (in millimeters)\nflipper_length_mm: length of the penguin’s flipper (in millimeters)\n\nAfter removing missing values, the data is standardized to have zero mean and unit variance. This ensures that both features contribute equally to the clustering process.\n\n\n\nImplementation: K-Means Algorithm\nThe K-Means algorithm proceeds in the following steps:\n\nInitialization: Randomly select k data points as initial centroids.\n\nAssignment: Assign each point to the nearest centroid.\n\nUpdate: Recalculate centroids as the mean of the points assigned to each cluster.\n\nRepeat: Iterate steps 2 and 3 until convergence (i.e., centroid movement is minimal).\n\nMy implementation replicates this logic and returns both the final cluster assignments and centroid locations. For demonstration purposes, I also visualize the algorithm’s iterative updates using a custom animation saved as a .gif file.\n\n\n\nK-Means Animation\nTo visualize how the centroids and cluster assignments evolve over time, I generated an animation that shows each update step. This allows us to “see” the algorithm in action as it converges.\n\n\n\nK-means animation\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Select relevant features and drop NA\nX = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(42)\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    for i in range(max_iters):\n        # Assign clusters\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        # Recalculate centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        # Convergence check\n        if np.allclose(centroids, new_centroids, atol=tol):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\nK-Means Animation To visualize how the centroids and cluster assignments evolve over time, I generated an animation that shows each update step. This allows us to “see” the algorithm in action as it converges.\n\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\ndef animate_kmeans(X, k):\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    ims = []\n    for _ in range(10):\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', alpha=0.6)\n        centers = ax.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x')\n        ims.append([scatter, centers])\n        centroids = new_centroids\n    ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=1000)\n    ani.save('kmeans_animation.gif', writer='pillow')\n    plt.close()\n\n\nanimate_kmeans(X_scaled, k=3)\n\n\nK-Means Clustering Animation\n\n\n\nK-means animation\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nK = range(2, 8)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X_scaled)\n    wcss.append(km.inertia_)  # Within-cluster sum of squares\n    silhouette_scores.append(silhouette_score(X_scaled, labels))\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K, wcss, marker='o')\nax[0].set_title('WCSS vs K')\nax[0].set_xlabel('Number of clusters')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K, silhouette_scores, marker='o')\nax[1].set_title('Silhouette Score vs K')\nax[1].set_xlabel('Number of clusters')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw4_questions.html#k-nearest-neighbors",
    "href": "blog/project1/hw4_questions.html#k-nearest-neighbors",
    "title": "Machine Learning in Action: K-Means and KNN",
    "section": "2. K Nearest Neighbors",
    "text": "2. K Nearest Neighbors\nIn this section, we explore the K-Nearest Neighbors (KNN) algorithm — a non-parametric method used for classification based on the proximity of data points in feature space. To illustrate how KNN handles complex decision boundaries, we create a synthetic dataset with a non-linear, sinusoidal boundary and use it to evaluate how KNN’s performance changes with different values of 𝑘, the number of neighbors considered.\n\nData Generation\nWe generated a synthetic dataset with two continuous features, x1 and x2, using uniform random sampling from the interval [-3, 3]. The binary outcome variable y is determined by whether a data point lies above or below a wiggly boundary, defined by the function: \\[\n\\text{boundary} = \\sin(4x_1) + x_1\n\\]\nThis setup creates a non-linearly separable classification problem, which is ideal for testing flexible, non-parametric models like K-Nearest Neighbors (KNN).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Generate training data\nnp.random.seed(42)\nn = 100\nx1_train = np.random.uniform(-3, 3, n)\nx2_train = np.random.uniform(-3, 3, n)\nboundary_train = np.sin(4 * x1_train) + x1_train\ny_train = (x2_train &gt; boundary_train).astype(int)\n\nHere, points above the curve are labeled 1, and those below it are labeled 0. This results in a non-linearly separable classification problem — an ideal case for KNN, which does not assume any specific functional form.\n\n\nData Visualization\nWe plotted the training dataset with:\n\nHorizontal axis: x1\nVertical axis: x2\nPoint color: binary class label y (red = 0, blue = 1)\nDashed black line: the true decision boundary defined by sin(4x1) + x1\n\nThis visual helps us intuitively understand the classification challenge — the data is not linearly separable, and the boundary curves multiple times, making the problem well-suited for KNN.\n\n\nTest Dataset\nTo evaluate model generalization, we created a new test dataset using the same method but a different random seed (np.random.seed(7)). This ensures that training and test data are independent but drawn from the same underlying distribution.\n\n# Plot training data\nplt.figure()\nplt.scatter(x1_train, x2_train, c=y_train, cmap='bwr', alpha=0.6, edgecolor='k')\nx1_vals = np.linspace(-3, 3, 300)\nplt.plot(x1_vals, np.sin(4 * x1_vals) + x1_vals, 'k--', label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Training Data with Wiggly Boundary')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Generate test data\nnp.random.seed(7)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\n\n\n\n\n\n\n\n\n\n\nKNN Implementation\nWe used scikit-learn’s KNeighborsClassifier to implement the KNN algorithm and tested it across values of 𝑘 ranging from 1 to 30. For each value of k, we:\n\nTrained the model on the training set\nPredicted labels for the test set\nCalculated classification accuracy\nThis allowed us to systematically examine how the number of neighbors affects model performance.\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Evaluate KNN performance for k = 1 to 30\nX_train = np.column_stack((x1_train, x2_train))\nX_test = np.column_stack((x1_test, x2_test))\naccuracy_scores = []\n\nfor k in range(1, 31):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_scores.append(accuracy * 100)\n\n# Plot k vs accuracy\nplt.figure()\nplt.plot(range(1, 31), accuracy_scores, marker='o')\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('Accuracy (%)')\nplt.title('KNN Accuracy on Test Data (k=1 to 30)')\nplt.grid(True)\nplt.show()\n\n# Find optimal k\noptimal_k = np.argmax(accuracy_scores) + 1\noptimal_accuracy = accuracy_scores[optimal_k - 1]\noptimal_k, optimal_accuracy\n\n\n\n\n\n\n\n\n\n\nResults\nThe following plot shows accuracy (%) versus k (number of neighbors):\n\nAccuracy peaks around k = 3\nOverfitting for small k (e.g. k=1)\nUnderfitting for very large k (e.g. k=30)\n\n🥇 Optimal K - Optimal k: 3 - Accuracy: 90.0% - This value of k offers the best generalization for this dataset."
  },
  {
    "objectID": "blog/project1/hw4_questions.html#k-means",
    "href": "blog/project1/hw4_questions.html#k-means",
    "title": "Machine Learning in Action: K-Means and KNN",
    "section": "",
    "text": "In this section, we apply the K-Means clustering algorithm to the Palmer Penguins dataset in order to explore natural groupings in the data without using species labels. K-Means is an unsupervised learning technique that partitions data into 𝑘 distinct clusters based on feature similarity.\nWe focus on two features — bill length and flipper length — and use both a custom implementation and scikit-learn’s built-in tools to perform clustering. To evaluate the quality of the clusters, we visualize the clustering process with an animation and apply quantitative metrics such as the Within-Cluster Sum of Squares (WCSS) and Silhouette Score to help determine the optimal number of clusters.\nThis analysis demonstrates how K-Means identifies patterns in biological measurements and how different values of 𝑘 affect the results.\n\n\nWe start by loading the Palmer Penguins dataset, a popular alternative to the Iris dataset, and selecting two continuous numerical features:\n\nbill_length_mm\nflipper_length_mm\n\nThese features are cleaned by removing rows with missing values (NaN). Then, we apply standard scaling using StandardScaler to normalize the feature values. This step is important because K-Means is sensitive to the scale of the features.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Select relevant features and drop NA\nX = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nWe implemented the K-Means algorithm from scratch using NumPy. Here’s how it works:\n\nInitialization: Randomly select k data points as initial cluster centroids.\nAssignment Step: Assign each point to the nearest centroid based on Euclidean distance.\nUpdate Step: Recalculate the centroids as the mean of all points assigned to each cluster.\nConvergence: Repeat steps 2–3 until centroids stabilize or the maximum number of iterations is reached.\n\nThis function returns the final cluster labels and centroids.\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    np.random.seed(42)\n    centroids = X[np.random.choice(range(len(X)), k, replace=False)]\n    for i in range(max_iters):\n        # Assign clusters\n        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)\n        # Recalculate centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        # Convergence check\n        if np.allclose(centroids, new_centroids, atol=tol):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n\n\n\nTo visualize the clustering process over iterations, we created an animation using matplotlib.animation. The animation shows how centroids shift and how points get reassigned over 10 iterations. Points are colored by their current cluster label. Centroids are marked with black x symbols.\n\nimport matplotlib.animation as animation\ndef animate_kmeans(X, k, filename='kmeans_converged.gif', tol=1e-4, max_iters=100):\n    centroids = X[np.random.choice(len(X), k, replace=False)]\n    history = []\n\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        new_centroids = []\n        for j in range(k):\n            cluster_points = X[labels == j]\n            if len(cluster_points) == 0:\n                new_centroids.append(X[np.random.choice(len(X))])\n            else:\n                new_centroids.append(cluster_points.mean(axis=0))\n        new_centroids = np.array(new_centroids)\n\n        history.append((labels.copy(), centroids.copy()))\n\n        # Check for convergence\n        max_movement = np.max(np.linalg.norm(new_centroids - centroids, axis=1))\n        if max_movement &lt; tol:\n            break\n\n        centroids = new_centroids\n\n    # Ensure we have enough frames for the animation\n    for _ in range(5):\n        history.append((labels.copy(), centroids.copy()))\n\n    # animation\n    fig, ax = plt.subplots()\n\n    def update(frame):\n        ax.clear()\n        labels, centroids = history[frame]\n        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', s=40, alpha=0.6)\n        ax.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100)\n        ax.set_title(f'Iteration {frame+1}')\n        ax.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n        ax.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n\n    ani = animation.FuncAnimation(fig, update, frames=len(history), interval=800)\n    ani.save(filename, writer='pillow')\n    plt.close()\nanimate_kmeans(X_scaled, k=3, filename='kmeans_converged.gif')\n\n\n\n\n\n\nK-means animation\n\n\n\n\n\n\nTo determine the optimal number of clusters k, we applied two common metrics:\n\nWCSS (Within-Cluster Sum of Squares): Measures compactness of the clusters.\n\n\nLower values indicate tighter clusters.\nWe look for an “elbow point” in the WCSS plot where the rate of improvement slows down.\n\n\nSilhouette Score: Measures how similar each point is to its own cluster compared to other clusters.\n\n\nRanges from -1 to 1.\nHigher values indicate better-defined clusters.\n\nWe computed both metrics for 𝑘 = 2 to 7\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nK = range(2, 8)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X_scaled)\n    wcss.append(km.inertia_)  # Within-cluster sum of squares\n    silhouette_scores.append(silhouette_score(X_scaled, labels))\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K, wcss, marker='o')\nax[0].set_title('WCSS vs K')\nax[0].set_xlabel('Number of clusters')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K, silhouette_scores, marker='o')\nax[1].set_title('Silhouette Score vs K')\nax[1].set_xlabel('Number of clusters')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()"
  }
]