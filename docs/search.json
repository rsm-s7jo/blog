[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\nSunghoon Jo\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nSunghoon \nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\nYour Name\nMay 1, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe key experimental conditions included:\n- A standard appeal letter (control group)\n- A matching donation letter, where the organization promised to match donations at ratios of 1:1, 2:1, or 3:1\n- A challenge grant letter, which emphasized reaching a target donation amount before a match would be triggered\n\nThe goal was to examine whether donors are more likely to give (and give more) when their donations are matched — and whether the size of the match ratio (e.g., 3:1 vs. 1:1) affects this behavior.\nTheir findings contributed to the literature on charitable giving by showing that matching gifts significantly increase response rates, though larger match ratios do not always yield proportionally larger effects.\nThis project seeks to replicate and explore these results using the original dataset provided by the authors. It will assess the impact of match treatments on both the likelihood and size of donations, using statistical tests and visualizations to mirror the findings of the original 2007 study."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe key experimental conditions included:\n- A standard appeal letter (control group)\n- A matching donation letter, where the organization promised to match donations at ratios of 1:1, 2:1, or 3:1\n- A challenge grant letter, which emphasized reaching a target donation amount before a match would be triggered\n\nThe goal was to examine whether donors are more likely to give (and give more) when their donations are matched — and whether the size of the match ratio (e.g., 3:1 vs. 1:1) affects this behavior.\nTheir findings contributed to the literature on charitable giving by showing that matching gifts significantly increase response rates, though larger match ratios do not always yield proportionally larger effects.\nThis project seeks to replicate and explore these results using the original dataset provided by the authors. It will assess the impact of match treatments on both the likelihood and size of donations, using statistical tests and visualizations to mirror the findings of the original 2007 study."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nprint(df.shape)\nprint(df.describe())\n\n(50083, 51)\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\nprint(df['treatment'].value_counts())\nprint(df['control'].value_counts())\n\ntreatment\n1    33396\n0    16687\nName: count, dtype: int64\ncontrol\n0    33396\n1    16687\nName: count, dtype: int64\n\n\n\nprint(df.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nVariables Tested\nTo verify the success of the random assignment, I conducted balance checks on two pre-treatment variables:\n\nmrm2: months since last donation\nfreq: number of prior donations\n\nThese variables were tested using both t-tests and linear regression.\n\n\nmrm2\n\nfrom scipy import stats\n\n# NaN removal\ndf_clean = df[['treatment', 'control', 'mrm2']].dropna()\n\n# separate treatment and control groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# T-test \nt_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)\nprint(f\"T-test result: t={t_stat:.3f}, p={p_val:.3f}\")\n\nT-test result: t=0.120, p=0.905\n\n\n\n\nfreq\n\nimport statsmodels.api as sm\n\n# freq: number of prior donations\ndf_freq = df[['treatment', 'freq']].dropna()\nfreq_treat = df_freq[df_freq['treatment'] == 1]['freq']\nfreq_control = df_freq[df_freq['treatment'] == 0]['freq']\nt_stat_freq, p_val_freq = stats.ttest_ind(freq_treat, freq_control, equal_var=False)\nprint(f\"T-test (freq): t={t_stat_freq:.3f}, p={p_val_freq:.3f}\")\n\nT-test (freq): t=-0.111, p=0.912\n\n\n\n\nLinear Regression\n\nimport statsmodels.formula.api as smf\n\n# Regression\nresult = smf.ols('mrm2 ~ treatment', data=df_clean).fit()\nprint(result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        20:42:13   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Regression\ndf_freq['intercept'] = 1\nmodel_freq = sm.OLS(df_freq['freq'], df_freq[['intercept', 'treatment']])\nresult_freq = model_freq.fit()\nprint(result_freq.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.912\nTime:                        20:42:13   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nResult\n\n\n\n\n\n\n\n\n\n\nVariable\nT-test (p-value)\nRegression Coefficient\nRegression (p-value)\nBalanced?\n\n\n\n\nmrm2\n0.905\n0.0137\n0.905\nYes\n\n\nfreq\n0.912\n-0.0120\n0.912\nYes\n\n\n\n\n\nFor both mrm2 and freq, we fail to reject the null hypothesis that the means are equal between the treatment and control groups. Both the t-tests and linear regressions produce p-values well above the 0.05 threshold, and the treatment coefficients are near zero.\nThis strongly suggests that the random assignment was successful in producing comparable groups at baseline. There is no evidence that individuals in the treatment group differed systematically in these pre-treatment characteristics.\n\n\nWhy This Matters (Table 1 Context)\n\nTable 1 in Karlan & List (2007) presents these balance checks to demonstrate the integrity of the experimental design. By showing no significant differences between groups before treatment, the authors reinforce the internal validity of the study: any post-treatment differences in outcomes can reasonably be attributed to the treatment itself.\nThese balance tests provide strong evidence that randomization worked as intended. The treatment and control groups appear statistically equivalent on observed baseline characteristics, supporting causal inference in subsequent analyses."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n# charity donation ratio\ndonation_rates = df.groupby('treatment')['gave'].mean()\n\n# bar plot\ndonation_rates.plot(kind='bar')\nplt.xticks([0, 1], ['Control', 'Treatment'], rotation=0)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# NaN removal\ndf_clean = df[['treatment', 'gave']].dropna()\n\n# T-test\ngave_treat = df_clean[df_clean['treatment'] == 1]['gave']\ngave_control = df_clean[df_clean['treatment'] == 0]['gave']\nt_stat, p_val = stats.ttest_ind(gave_treat, gave_control, equal_var=False)\nprint(f\"T-test result: t = {t_stat:.3f}, p = {p_val:.3f}\")\n\n# linear regression: gave ~ treatment\ndf_clean['intercept'] = 1\nmodel = sm.OLS(df_clean['gave'], df_clean[['intercept', 'treatment']])\nresult = model.fit()\nprint(result.summary())\n\nT-test result: t = 3.209, p = 0.001\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        20:42:13   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThere is a statistically significant difference in donation rates between groups. The treatment group had a higher likelihood of donating, and the p-value indicates this difference is unlikely to be due to chance.\nThe regression confirms the t-test: being in the treatment group increases the donation probability by 0.42 percentage points. Though small in magnitude, this effect is statistically significant, indicating that matched donations encourage giving.\n\nimport statsmodels.api as sm\n\n# Probit model\nprobit_model = sm.Probit(df_clean['gave'], df_clean[['intercept', 'treatment']])\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        20:42:13   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThe positive and significant coefficient from the probit regression supports the earlier findings: Being offered a matching donation increases the probability of giving. This matches Table 3, Column 1 in Karlan & List (2007), confirming the replication.\n\nHuman Behavior Insight\nThese results show that even a small external incentive — like a matching donation — can nudge people toward giving. It reinforces the idea that donors are influenced not just by personal motivation, but also by context and structure of giving opportunities.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# 1:1 vs 2:1\nt1 = df[df['ratio'] == 1]['gave'].dropna()\nt2 = df[df['ratio2'] == 1]['gave'].dropna()\nt_stat_1_2, p_val_1_2 = stats.ttest_ind(t1, t2, equal_var=False)\n\n# 2:1 vs 3:1\nt3 = df[df['ratio3'] == 1]['gave'].dropna()\nt_stat_2_3, p_val_2_3 = stats.ttest_ind(t2, t3, equal_var=False)\n\nprint(f\"1:1 vs 2:1 → t = {t_stat_1_2:.3f}, p = {p_val_1_2:.3f}\")\nprint(f\"2:1 vs 3:1 → t = {t_stat_2_3:.3f}, p = {p_val_2_3:.3f}\")\n\n1:1 vs 2:1 → t = -0.965, p = 0.335\n2:1 vs 3:1 → t = -0.050, p = 0.960\n\n\nThere is no statistically significant difference in donation rates between the 1:1 and 2:1 match groups, or between 2:1 and 3:1 groups. These results suggest that increasing the match ratio does not meaningfully affect whether people donate.\n\n# 1:1 dummy variable\ndf['ratio1'] = ((df['ratio'] == 1) & (df['ratio2'] != 1) & (df['ratio3'] != 1)).astype(int)\n\nmodel = sm.OLS(df['gave'], sm.add_constant(df[['ratio1', 'ratio2', 'ratio3']].fillna(0)))\nresult = model.fit()\nprint(result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        20:42:13   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nControlling for other match types, 2:1 and 3:1 match ratios are statistically significant predictors of giving — and lead to ~0.48–0.49 percentage point increases in donation rate compared to the baseline. However, the magnitude of the difference between match levels is small and likely not practically meaningful.\n\nrate_1_1 = df[df['ratio1'] == 1]['gave'].mean()\nrate_2_1 = df[df['ratio2'] == 1]['gave'].mean()\nrate_3_1 = df[df['ratio3'] == 1]['gave'].mean()\n\nprint(f\"2:1 - 1:1 = {rate_2_1 - rate_1_1:.4f}\")\nprint(f\"3:1 - 2:1 = {rate_3_1 - rate_2_1:.4f}\")\n\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\nWhile increasing the match ratio leads to slightly higher donation rates, the marginal gains from going beyond 1:1 are extremely small. This aligns with the paper’s “figures suggest” comment — indicating that larger matches might not be more effective in practice.\nThese results show that although higher match ratios appear to encourage slightly more giving, the differences are tiny and inconsistent. The data suggest that simply offering a match (vs. no match) matters, but offering bigger matches (e.g., 3:1 instead of 2:1) doesn’t have a clear benefit. People may be motivated by the presence of a match, but not necessarily its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport statsmodels.formula.api as smf\nresult_amt = smf.ols('amount ~ treatment', data=df).fit()\nprint(result_amt.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        20:42:13   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nBeing in the treatment group is associated with an average donation increase of ~$0.15, but this result is not statistically significant at the 5% level (p ≈ 0.06). This suggests weak evidence that the matching offer may slightly increase giving amounts on average, but the effect is small and uncertain.\n\ndonors = df[df['gave'] == 1]\nresult_amt_donors = smf.ols('amount ~ treatment', data=donors).fit()\nprint(result_amt_donors.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        20:42:13   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAmong people who did give, being in the treatment group is associated with a slightly lower donation, but the difference is not statistically significant. This means the matching offer does not appear to increase the size of donations from those already motivated to give.\n\nimport matplotlib.pyplot as plt\n\ndonors_t = donors[donors['treatment'] == 1]\ndonors_c = donors[donors['treatment'] == 0]\n\nplt.hist(donors_c['amount'], bins=30, alpha=0.6)\nplt.axvline(donors_c['amount'].mean(), color='red', linestyle='dashed', label='Mean')\nplt.title(\"Donation Amounts - Control Group\")\nplt.legend()\nplt.show()\n\nplt.hist(donors_t['amount'], bins=30, alpha=0.6)\nplt.axvline(donors_t['amount'].mean(), color='red', linestyle='dashed', label='Mean')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth treatment and control groups have right-skewed donation distributions (long tail of large donations). The average donation is marked with a red dashed line on each plot. Visually, the distributions are very similar, supporting the regression results.\nThese results suggest that:\n\nOffering a match may make someone slightly more likely to give,\nBut it does not meaningfully affect how much they give.\n\nThis implies that match offers work more by encouraging action, not by increasing generosity among existing donors. In other words: people give because of the match — not more because of the match."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# seed\nnp.random.seed(42)\n\n# difference between 10_000 samples\nn_total = 100000\ncontrol = np.random.binomial(1, 0.018, n_total)\ntreatment = np.random.binomial(1, 0.022, 10000)\n\n# slicing\ncontrol = control[:10000]\n\n# difference from each sample\ndifferences = treatment - control\n\n# cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, 10001)\n\n# plot\nplt.plot(cumulative_avg, label='Cumulative Difference')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Avg Difference')\nplt.title('Law of Large Numbers in Action')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the donation rate difference converged toward the true difference of 0.004, confirming the Law of Large Numbers in practice.\n\n\nCentral Limit Theorem\n\nimport seaborn as sns\n\nsample_sizes = [50, 200, 500, 1000]\ntrue_diff = 0.004\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n    for _ in range(1000):\n        c = np.random.binomial(1, 0.018, n)\n        t = np.random.binomial(1, 0.022, n)\n        avg_diffs.append(np.mean(t) - np.mean(c))\n    \n    sns.histplot(avg_diffs, kde=True, ax=axes[i], bins=30)\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True diff = 0.004')\n    axes[i].set_title(f'Sample size: {n}')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs the sample size increased, the distribution of the mean difference became more tightly centered around the true value and more bell-shaped, illustrating the Central Limit Theorem. Zero moved from being near the center (at small n) to clearly in the tail (at larger n), showing the power of sample size in detecting treatment effects."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "qenv/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "qenv/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Sunghoon's Blog",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sunghoon",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "blog/project1/index.html#sub-header",
    "href": "blog/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Resume"
  },
  {
    "objectID": "blog/project1/hw2_questions.html",
    "href": "blog/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "blog/project1/hw2_questions.html#airbnb-case-study",
    "href": "blog/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe use data on 40,000 Airbnb listings in NYC, scraped in March 2017.\nWe assume the number of reviews is a reasonable proxy for the number of bookings.\n\n\n\nData Load and Overview\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Airbnb data\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n# Histogram of number of reviews\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Listings\")\nplt.xlim(0, 100)  # Clip for visibility\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot: reviews by room type\nsns.boxplot(data=df, x='room_type', y='number_of_reviews')\nplt.title(\"Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Drop NA values in key columns\ndf_model = df[[\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]].dropna()\n\n\n\nPrepare Data for Modeling\n\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n# One-hot encode categorical vars\nX = df_model.copy()\nX['log_price'] = np.log1p(X['price'])  # stabilize price\nX['instant_bookable'] = (X['instant_bookable'] == 't').astype(int)\n\n# One-hot encode room_type\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\n\n# Define target and feature matrix\ny = X['number_of_reviews'].values\nX = X.drop(columns=['number_of_reviews'])\nX = pd.concat([pd.Series(1, index=X.index, name='intercept'), X], axis=1)\nX_mat = X.values.astype(float)\n\n\n\nEstimate Poisson Regression Model\n\nimport statsmodels.api as sm\n\nglm_poisson = sm.GLM(y, X_mat, family=sm.families.Poisson())\nglm_result = glm_poisson.fit()\n\nglm_result.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30148\n\n\nModel Family:\nPoisson\nDf Model:\n11\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2215e+05\n\n\nDate:\nThu, 01 May 2025\nDeviance:\n9.2283e+05\n\n\nTime:\n21:41:32\nPearson chi2:\n1.66e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.7237\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.3093\n0.026\n87.955\n0.000\n2.258\n2.361\n\n\nx1\n5.049e-05\n3.94e-07\n128.131\n0.000\n4.97e-05\n5.13e-05\n\n\nx2\n-0.1097\n0.004\n-28.467\n0.000\n-0.117\n-0.102\n\n\nx3\n0.0565\n0.002\n27.238\n0.000\n0.052\n0.061\n\n\nx4\n-0.0009\n2.54e-05\n-36.640\n0.000\n-0.001\n-0.001\n\n\nx5\n0.1073\n0.002\n71.426\n0.000\n0.104\n0.110\n\n\nx6\n-0.1015\n0.002\n-61.332\n0.000\n-0.105\n-0.098\n\n\nx7\n-0.0774\n0.002\n-42.335\n0.000\n-0.081\n-0.074\n\n\nx8\n0.3545\n0.003\n122.520\n0.000\n0.349\n0.360\n\n\nx9\n0.2999\n0.005\n58.052\n0.000\n0.290\n0.310\n\n\nx10\n0.1221\n0.003\n35.142\n0.000\n0.115\n0.129\n\n\nx11\n-0.0297\n0.009\n-3.208\n0.001\n-0.048\n-0.012\n\n\n\n\n\n\nInterpretation of Coefficients\nCoefficients in a Poisson model represent log changes in the expected count of reviews.\nFor example, a coefficient of 0.3 on room_type_Private room implies that private rooms are expected to receive ~35% more reviews than shared rooms, all else equal.\n\n\n\nPredicted Review Counts by Instant Bookability\n\nX0 = X.copy()\nX0['instant_bookable'] = 0\nX1 = X.copy()\nX1['instant_bookable'] = 1\n\nX0_mat = X0[X.columns].values.astype(float)\nX1_mat = X1[X.columns].values.astype(float)\n\ny_pred_0 = np.exp(X0_mat @ glm_result.params)\ny_pred_1 = np.exp(X1_mat @ glm_result.params)\n\nprint(\"Average predicted reviews:\")\nprint(f\"Not instant bookable: {y_pred_0.mean():.2f}\")\nprint(f\"Instant bookable: {y_pred_1.mean():.2f}\")\n\nAverage predicted reviews:\nNot instant bookable: 19.56\nInstant bookable: 27.88"
  },
  {
    "objectID": "blog/project1/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "href": "blog/project1/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\nWe now extend the simple Poisson model to a Poisson regression model. Let:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = e^{X_i'\\beta}\n\\]\nThis allows the expected count to depend on firm-level covariates, such as age, region, and Blueprinty customer status.\n\n\nLog-Likelihood Function\n\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define log-likelihood function\ndef poisson_log_likelihood(beta, y, X):\n    beta = np.atleast_1d(beta) \n    lin_pred = X @ beta  # Xβ\n    lam = np.exp(lin_pred)  # inverse link: exp\n    return np.sum(y * lin_pred - lam - gammaln(y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy import optimize\n# Ignore warnings\nnp.seterr(over='ignore', invalid='ignore')\n\n# Create design matrix X\ndf['age_squared'] = df['age'] ** 2\n\nX = df[['age', 'age_squared', 'iscustomer']].copy()\n\n# One-hot encode regions (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=df.index, name='intercept'), X, region_dummies], axis=1)\n\ny = df['patents'].values\nX_mat = X.values.astype(float) \n\n\n# Negative log-likelihood\ndef neg_loglik(beta):\n    return -poisson_log_likelihood(beta, y, X_mat)\n\n# Initial guess\nbeta_init = np.zeros(X_mat.shape[1])\n\n# Minimize\nresult = optimize.minimize(neg_loglik, beta_init, method='BFGS')\nbeta_hat = result.x\nhessian = result.hess_inv\n\n# Standard errors\nse_beta = np.sqrt(np.diag(hessian))\n\n# Summary table\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': se_beta\n}, index=X.columns)\n\ncoef_table.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n0.0\n1.0\n\n\nage\n0.0\n1.0\n\n\nage_squared\n0.0\n1.0\n\n\niscustomer\n0.0\n1.0\n\n\nNortheast\n0.0\n1.0\n\n\nNorthwest\n0.0\n1.0\n\n\nSouth\n0.0\n1.0\n\n\nSouthwest\n0.0\n1.0\n\n\n\n\n\n\n\nBased on the model, using Blueprinty is associated with an average increase of approximately X patents per firm (replace X with actual value). Because this is a log-linear model, we interpret the coefficients multiplicatively on the rate scale.\n\nInterpret Blueprinty Effect (Using Counterfactual Prediction)\n\n# Create X0 (all non-customers), X1 (all customers)\nX0 = X.copy()\nX0['iscustomer'] = 0\nX0 = X0[X.columns]\nX1 = X.copy()\nX1['iscustomer'] = 1\nX1 = X1[X.columns]\n\nX0_mat = X0.values.astype(float)\nX1_mat = X1.values.astype(float)\n\ny_pred_0 = np.exp(X0_mat @ beta_hat)\ny_pred_1 = np.exp(X1_mat @ beta_hat)\n\n# Average difference\neffect = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Estimated average effect of Blueprinty: {effect:.2f} more patents per firm\")\n\nEstimated average effect of Blueprinty: 0.00 more patents per firm"
  },
  {
    "objectID": "blog/project1/hw2_questions.html#airbnb-case-study-1",
    "href": "blog/project1/hw2_questions.html#airbnb-case-study-1",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\nWe use data on 40,000 Airbnb listings in NYC, scraped in March 2017.\nWe assume the number of reviews is a reasonable proxy for the number of bookings.\n\n\nData Load and Overview\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Airbnb data\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nExploratory Data Analysis (EDA)\n\n# Histogram of number of reviews\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Listings\")\nplt.xlim(0, 100)  # Clip for visibility\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot: reviews by room type\nsns.boxplot(data=df, x='room_type', y='number_of_reviews')\nplt.title(\"Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Drop NA values in key columns\ndf_model = df[[\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]].dropna()\n\n\n\nPrepare Data for Modeling\n\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n# One-hot encode categorical vars\nX = df_model.copy()\nX['log_price'] = np.log1p(X['price'])  # stabilize price\nX['instant_bookable'] = (X['instant_bookable'] == 't').astype(int)\n\n# One-hot encode room_type\nX = pd.get_dummies(X, columns=['room_type'], drop_first=True)\n\n# Define target and feature matrix\ny = X['number_of_reviews'].values\nX = X.drop(columns=['number_of_reviews'])\nX = pd.concat([pd.Series(1, index=X.index, name='intercept'), X], axis=1)\nX_mat = X.values.astype(float)\n\n\n\nEstimate Poisson Regression Model\n\nimport statsmodels.api as sm\n\nglm_poisson = sm.GLM(y, X_mat, family=sm.families.Poisson())\nglm_result = glm_poisson.fit()\n\nglm_result.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30148\n\n\nModel Family:\nPoisson\nDf Model:\n11\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2215e+05\n\n\nDate:\nThu, 01 May 2025\nDeviance:\n9.2283e+05\n\n\nTime:\n21:39:24\nPearson chi2:\n1.66e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.7237\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.3093\n0.026\n87.955\n0.000\n2.258\n2.361\n\n\nx1\n5.049e-05\n3.94e-07\n128.131\n0.000\n4.97e-05\n5.13e-05\n\n\nx2\n-0.1097\n0.004\n-28.467\n0.000\n-0.117\n-0.102\n\n\nx3\n0.0565\n0.002\n27.238\n0.000\n0.052\n0.061\n\n\nx4\n-0.0009\n2.54e-05\n-36.640\n0.000\n-0.001\n-0.001\n\n\nx5\n0.1073\n0.002\n71.426\n0.000\n0.104\n0.110\n\n\nx6\n-0.1015\n0.002\n-61.332\n0.000\n-0.105\n-0.098\n\n\nx7\n-0.0774\n0.002\n-42.335\n0.000\n-0.081\n-0.074\n\n\nx8\n0.3545\n0.003\n122.520\n0.000\n0.349\n0.360\n\n\nx9\n0.2999\n0.005\n58.052\n0.000\n0.290\n0.310\n\n\nx10\n0.1221\n0.003\n35.142\n0.000\n0.115\n0.129\n\n\nx11\n-0.0297\n0.009\n-3.208\n0.001\n-0.048\n-0.012\n\n\n\n\n\n\nInterpretation of Coefficients\nCoefficients in a Poisson model represent log changes in the expected count of reviews.\nFor example, a coefficient of 0.3 on room_type_Private room implies that private rooms are expected to receive ~35% more reviews than shared rooms, all else equal.\n\n\n\nPredicted Review Counts by Instant Bookability\n\nX0 = X.copy()\nX0['instant_bookable'] = 0\nX1 = X.copy()\nX1['instant_bookable'] = 1\n\nX0_mat = X0[X.columns].values.astype(float)\nX1_mat = X1[X.columns].values.astype(float)\n\ny_pred_0 = np.exp(X0_mat @ glm_result.params)\ny_pred_1 = np.exp(X1_mat @ glm_result.params)\n\nprint(\"Average predicted reviews:\")\nprint(f\"Not instant bookable: {y_pred_0.mean():.2f}\")\nprint(f\"Instant bookable: {y_pred_1.mean():.2f}\")\n\nAverage predicted reviews:\nNot instant bookable: 19.56\nInstant bookable: 27.88"
  },
  {
    "objectID": "blog/project1/hw2_questions.html#data-load-and-overview",
    "href": "blog/project1/hw2_questions.html#data-load-and-overview",
    "title": "Poisson Regression Examples",
    "section": "Data Load and Overview",
    "text": "Data Load and Overview\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Blueprinty data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Histogram comparing patent counts\nsns.histplot(data=df, x='patents', hue='iscustomer', kde=True, element='step')\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Firm Count\")\nplt.show()\n\n\n\n\n\n\n\n\nWe see a clear shift in the distribution of patent counts between Blueprinty customers and non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare mean number of patents\ndf.groupby('iscustomer')['patents'].mean().round(2)\n\n# Boxplot of firm age\nsns.boxplot(data=df, x='iscustomer', y='age')\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Region comparison\npd.crosstab(df['region'], df['iscustomer'], normalize='index') \\\n    .round(2) * 100\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.0\n17.0\n\n\nNortheast\n45.0\n55.0\n\n\nNorthwest\n84.0\n16.0\n\n\nSouth\n82.0\n18.0\n\n\nSouthwest\n82.0\n18.0\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLog-Likelihood Function\nWe assume that the number of patents awarded to each firm over the last 5 years follows a Poisson distribution. The Poisson density is defined as:\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) for stability\n\ndef poisson_log_likelihood(lmbda, y):\n    # log-likelihood of Poisson for a single value or array\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\nAs expected, the log-likelihood is maximized when λ = Y.\n\nimport matplotlib.pyplot as plt\n\n# Example: one firm with 4 patents\ny_obs = 4\nlambdas = np.linspace(0.1, 10, 200)\nlogliks = [poisson_log_likelihood(lmbda, y_obs) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.title(\"Log-Likelihood of Poisson for Y = 4\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(x=y_obs, color='red', linestyle='--', label='Y = λ MLE')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimator for the Poisson Model\nTaking the derivative of the log-likelihood function and setting it to zero gives:\n\\[\n\\frac{d\\ell}{d\\lambda} = -1 + \\frac{Y}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = Y\n\\]\nThis makes intuitive sense, since the mean of a Poisson distribution is equal to its rate parameter λ.\nIf we observe multiple firms, and let \\(Y_1, Y_2, ..., Y_n\\) be the number of patents for each firm, the MLE becomes:\n\\[\n\\lambda_{\\text{MLE}} = \\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n\\]\nThis confirms that the sample mean is the MLE for the Poisson parameter λ.\n\nfrom scipy import optimize\n\nY = df['patents'].values\n\n# Negative log-likelihood (since most optimizers minimize)\ndef neg_loglik(lmbda):\n    return -poisson_log_likelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE estimate of λ: {lambda_mle:.3f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.3f}\")\n\nMLE estimate of λ: 3.685\nSample mean of Y: 3.685\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty."
  }
]