---
title: "Multinomial Logit Model"
author: "Sunghoon Jo"
date: today
---


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{python}
import pandas as pd
import numpy as np
import random
from itertools import product

# Set seed for reproducibility
np.random.seed(123)
random.seed(123)

# Define attributes
brand = ["N", "P", "H"] # Netflix, Prime, Hulu
ad = ["Yes", "No"]
# R's seq(8, 32, by=4) is a sequence from 8 to 32 incrementing by 4
price = np.arange(8, 33, 4) # 33 is exclusive, so it goes up to 32

# Generate all possible profiles
# Equivalent to R's expand.grid
profiles = pd.DataFrame(list(product(brand, ad, price)), columns=['brand', 'ad', 'price'])
m = len(profiles) # Number of rows in profiles

# Assign part-worth utilities (true parameters)
b_util = {"N": 1.0, "P": 0.5, "H": 0}
a_util = {"Yes": -0.8, "No": 0.0}
p_util = lambda p: -0.1 * p

# Number of respondents, choice tasks, and alternatives per task
n_peeps = 100
n_tasks = 10
n_alts = 3

# Function to simulate one respondentâ€™s data
def sim_one(id):
    datlist = []

    # Loop over choice tasks
    for t in range(1, n_tasks + 1):
        # Randomly sample 3 alternatives (better practice would be to use a design)
        # Equivalent to R's profiles[sample(m, size=n_alts), ]
        sampled_indices = random.sample(range(m), k=n_alts)
        dat = profiles.iloc[sampled_indices].copy() # Use .copy() to avoid SettingWithCopyWarning
        
        # Add resp and task columns
        dat['resp'] = id
        dat['task'] = t
        
        # Compute deterministic portion of utility
        # Equivalent to R's b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price)
        dat['v'] = dat.apply(lambda row: b_util[row['brand']] + a_util[row['ad']] + p_util(row['price']), axis=1)
        
        # Add Gumbel noise (Type I extreme value)
        # Equivalent to R's -log(-log(runif(n_alts)))
        # Using inverse transform sampling for Gumbel distribution from uniform(0,1)
        dat['e'] = -np.log(-np.log(np.random.rand(n_alts))) 
        dat['u'] = dat['v'] + dat['e']
        
        # Identify chosen alternative
        # Equivalent to R's as.integer(dat$u == max(dat$u))
        dat['choice'] = (dat['u'] == dat['u'].max()).astype(int)
        
        # Store task
        datlist.append(dat)
    
    # Combine all tasks for one respondent
    # Equivalent to R's do.call(rbind, datlist)
    return pd.concat(datlist, ignore_index=True)

# Simulate data for all respondents
# Equivalent to R's do.call(rbind, lapply(1:n_peeps, sim_one))
conjoint_data_list = [sim_one(i) for i in range(1, n_peeps + 1)]
conjoint_data = pd.concat(conjoint_data_list, ignore_index=True)

# Remove values unobservable to the researcher
# Equivalent to R's conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]
conjoint_data = conjoint_data.loc[:, ["resp", "task", "brand", "ad", "price", "choice"]]

# Print results
print("Generated conjoint_data head:")
print(conjoint_data.head())
print("\nGenerated conjoint_data info:")
conjoint_data.info()
print(f"\nTotal number of respondents: {conjoint_data['resp'].nunique()}")
print(f"Total number of choice tasks (per respondent): {conjoint_data['task'].nunique()}")
print(f"Total number of alternatives (per task): {conjoint_data.groupby(['resp', 'task']).size().mean()}")
```
::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.
```{python}
import pandas as pd
import numpy as np
import random
from itertools import product

conjoint_data = pd.read_csv('conjoint_data.csv', index_col=0)


# Create dummy variables for categorical attributes
# 'H' (Hulu) and 'No' (ad-free) are the reference categories in the utility function.
# So, we will create dummies for 'N' (Netflix), 'P' (Prime), and 'Yes' (Ads).
# pd.get_dummies will create columns like 'brand_N', 'brand_P', 'brand_H', 'ad_Yes', 'ad_No'.
# We will then select and rename the specific columns that correspond to our beta parameters.

df_prep = conjoint_data.copy()

# Convert 'brand' and 'ad' into dummy variables
# We will create all dummies first, then select the ones corresponding to the non-reference categories
df_prep = pd.get_dummies(df_prep, columns=['brand', 'ad'], drop_first=False)

# Rename the dummy columns to match the utility function's variable names
# For 'brand': 'N' -> 'Netflix', 'P' -> 'Prime'
# For 'ad': 'Yes' -> 'Ads'
df_prep.rename(columns={
    'brand_N': 'Netflix',
    'brand_P': 'Prime',
    'ad_Yes': 'Ads'
}, inplace=True)

# Drop the reference category dummy variables as their coefficients are implicitly zero
# and they should not be included in the X matrix for direct estimation.
# The original problem statement indicates 'Hulu' and 'Ad-free' as reference.
df_prep = df_prep.drop(columns=['brand_H', 'ad_No'])

# Verify the prepared data structure
print("\n--- Prepared df_prep head (with dummy variables for estimation) ---")
print(df_prep.head())
print("\n--- Prepared df_prep info ---")
df_prep.info()

# Final check: Ensure the columns for estimation are present
# We need 'Netflix', 'Prime', 'Ads', 'price', 'resp', 'task', 'choice'
# The columns 'resp' and 'task' are crucial for grouping alternatives within a choice set.

print("\ndf_prep DataFrame prepared for estimation.")

```


## 4. Estimation via Maximum Likelihood


::::
```{python}
from scipy.optimize import minimize # Import only the minimize function

# Define the log-likelihood function
# beta_vector: [beta_netflix, beta_prime, beta_ads, beta_price]
def mnl_log_likelihood(beta_vector, data):
    # IMPORTANT: Create a copy of the data to avoid modifying the original DataFrame
    # in subsequent calls by the optimizer.
    data = data.copy()

    # Extract relevant columns for the X matrix. Their order must match beta_vector.
    # Expected column names in 'data': 'Netflix', 'Prime', 'Ads', 'price'
    X = data[['Netflix', 'Prime', 'Ads', 'price']].values

    # --- CRITICAL FIX: Explicitly convert X to float64 type ---
    # This ensures X is a proper float NumPy array for np.dot operation,
    # preventing the 'float' object has no attribute 'exp' error.
    X = X.astype(np.float64)
    # --- End of fix ---

    # Calculate deterministic utility V_ij = x_j'beta
    V = np.dot(X, beta_vector)
    
    # Calculate exp(V) for all alternatives
    # The previous error occurred because V was not a suitable NumPy array for np.exp.
    # Ensuring numeric types in df_prep and explicitly converting X should fix this.
    data['exp_V'] = np.exp(V)

    # Group by 'resp' and 'task' to get the sum of exp(V) for each choice set.
    # This sum will serve as the denominator for the probabilities.
    sum_exp_V_per_task = data.groupby(['resp', 'task'])['exp_V'].transform('sum')

    # Calculate probabilities P_i(j) = exp(V_ij) / sum(exp(V_ik))
    # Add a small constant (1e-9) to the denominator for numerical stability, to prevent division by zero.
    probabilities = data['exp_V'] / (sum_exp_V_per_task + 1e-9)

    # Calculate log-likelihood: sum_i sum_j (delta_ij * log(P_i(j)))
    # delta_ij is the 'choice' column (1 if chosen, 0 otherwise).
    # Only terms where choice=1 will contribute to the sum.
    log_probabilities = np.log(probabilities)
    
    # Sum the log-probabilities for the chosen alternatives.
    total_log_likelihood = np.sum(np.where(data['choice'] == 1, log_probabilities, 0))

    # We aim to maximize this likelihood, so for minimization, we return the negative log-likelihood.
    return -total_log_likelihood

# --- MLE Estimation ---

# Initial guess for beta parameters
# Order: [beta_netflix, beta_prime, beta_ads, beta_price]
initial_betas = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64) # Explicitly set dtype

# Optimize the negative log-likelihood function
# 'args' passes additional arguments (our data, i.e., df_prep) to the log-likelihood function.
result = minimize(mnl_log_likelihood, initial_betas, args=(df_prep,), method='BFGS') # BFGS is a good general-purpose optimizer.

# Print optimization results
print("\n--- MLE Optimization Results ---")
print(result)

# Extract MLEs
mle_betas = result.x
print("\nMLE Beta Parameters:")
print(f"Beta_Netflix: {mle_betas[0]:.4f}")
print(f"Beta_Prime:   {mle_betas[1]:.4f}")
print(f"Beta_Ads:     {mle_betas[2]:.4f}")
print(f"Beta_Price:   {mle_betas[3]:.4f}")

# --- Standard Errors and Confidence Intervals ---

# The 'hess_inv' attribute from scipy.optimize.OptimizeResult is the inverse Hessian of the objective function
# (which is the negative log-likelihood here). For the BFGS method, this directly provides the asymptotic covariance matrix of the MLEs.
cov_matrix = result.hess_inv

# Standard errors are the square root of the diagonal elements of the covariance matrix.
std_errors = np.sqrt(np.diag(cov_matrix))

# Construct 95% Confidence Intervals
# For large samples, MLEs are asymptotically normally distributed.
# CI = Estimate +/- Z_alpha/2 * Standard Error
# For a 95% CI, the Z-value (Z_alpha/2 for a two-tailed test) is approximately 1.96.
z_value = 1.96

conf_intervals = []
for i in range(len(mle_betas)):
    lower_bound = mle_betas[i] - z_value * std_errors[i]
    upper_bound = mle_betas[i] + z_value * std_errors[i]
    conf_intervals.append((lower_bound, upper_bound))

# Print results with standard errors and confidence intervals
param_names = ['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price']
print("\n--- MLE Parameters with Standard Errors and 95% Confidence Intervals ---")
for i, name in enumerate(param_names):
    print(f"{name}: {mle_betas[i]:.4f} (Std Err: {std_errors[i]:.4f}, 95% CI: [{conf_intervals[i][0]:.4f}, {conf_intervals[i][1]:.4f}])")

# Compare with true parameters (for mock data scenarios)
true_betas = np.array([1.0, 0.5, -0.8, -0.1])
print("\n--- True Parameters ---")
print(f"True Beta_Netflix: {true_betas[0]:.4f}")
print(f"True Beta_Prime:   {true_betas[1]:.4f}")
print(f"True Beta_Ads:     {true_betas[2]:.4f}")
print(f"True Beta_Price:   {true_betas[3]:.4f}")
```
::::

## 5. Estimation via Bayesian Methods

```{python}
import pandas as pd
import numpy as np
import random
from itertools import product
from scipy.optimize import minimize
from scipy.stats import norm
import matplotlib.pyplot as plt
import seaborn as sns

# --- Ensure df_prep is defined (from Section 3) ---
# This block is for self-contained execution. In a live notebook, you'd run Section 3 first.
try:
    df_prep.head()
except NameError:
    print("df_prep DataFrame is not defined. Running mock data generation and preparation.")
    np.random.seed(123)
    random.seed(123)
    brand_options = ["N", "P", "H"]
    ad_options = ["Yes", "No"]
    price_options = np.arange(8, 33, 4)
    n_peeps = 100
    n_tasks = 10
    n_alts = 3
    mock_data_list = []
    for resp_id in range(1, n_peeps + 1):
        for task_id in range(1, n_tasks + 1):
            sampled_profiles = pd.DataFrame(list(product(brand_options, ad_options, price_options)), columns=['brand', 'ad', 'price']).sample(n=n_alts, random_state=resp_id*task_id)
            sampled_profiles['resp'] = resp_id
            sampled_profiles['task'] = task_id
            b_util_mock = {"N": 1.0, "P": 0.5, "H": 0}
            a_util_mock = {"Yes": -0.8, "No": 0.0}
            p_util_mock = lambda p: -0.1 * p
            sampled_profiles['v_mock'] = sampled_profiles.apply(
                lambda row: b_util_mock[row['brand']] + a_util_mock[row['ad']] + p_util_mock(row['price']), axis=1
            )
            sampled_profiles['e_mock'] = -np.log(-np.log(np.random.rand(n_alts)))
            sampled_profiles['u_mock'] = sampled_profiles['v_mock'] + sampled_profiles['e_mock']
            sampled_profiles['choice'] = (sampled_profiles['u_mock'] == sampled_profiles['u_mock'].max()).astype(int)
            mock_data_list.append(sampled_profiles[['resp', 'task', 'brand', 'ad', 'price', 'choice']])
    conjoint_data = pd.concat(mock_data_list, ignore_index=True)
    
    df_prep = conjoint_data.copy()
    df_prep = pd.get_dummies(df_prep, columns=['brand', 'ad'], drop_first=False)
    df_prep.rename(columns={
        'brand_N': 'Netflix',
        'brand_P': 'Prime',
        'ad_Yes': 'Ads'
    }, inplace=True)
    df_prep = df_prep.drop(columns=['brand_H', 'ad_No'])
    cols_to_ensure_numeric = ['Netflix', 'Prime', 'Ads', 'price', 'resp', 'task', 'choice']
    for col in cols_to_ensure_numeric:
        df_prep[col] = pd.to_numeric(df_prep[col], errors='coerce')
    initial_rows_count = df_prep.shape[0]
    df_prep.dropna(subset=cols_to_ensure_numeric, inplace=True)
    if df_prep.shape[0] < initial_rows_count:
        print(f"Warning: Dropped {initial_rows_count - df_prep.shape[0]} rows due to non-numeric values or NaNs during mock data prep.")
    print("df_prep DataFrame prepared from mock data.")


# Define the log-likelihood function (re-using from MLE section)
# beta_vector: [beta_netflix, beta_prime, beta_ads, beta_price]
def mnl_log_likelihood(beta_vector, data):
    data = data.copy() # Create a copy to avoid modifying the original DataFrame

    X = data[['Netflix', 'Prime', 'Ads', 'price']].values
    X = X.astype(np.float64) # Ensure X is float64 for np.dot

    V = np.dot(X, beta_vector)
    
    # Clip V to prevent extreme values that cause inf/nan in exp(V)
    V_clipped = np.clip(V, -500, 500) 
    
    # Assign exp_V directly to a new column in the data DataFrame
    data['exp_V'] = np.exp(V_clipped)

    # Group by resp and task to get sum of exp(V) for each choice set
    sum_exp_V_per_task = data.groupby(['resp', 'task'])['exp_V'].transform('sum')

    # Add a small constant for numerical stability to prevent division by zero
    probabilities = data['exp_V'] / (sum_exp_V_per_task + 1e-9)

    # Add small constant to probabilities before log to avoid log(0) which is -inf
    log_probabilities = np.log(probabilities + 1e-9) 

    total_log_likelihood = np.sum(np.where(data['choice'] == 1, log_probabilities, 0))

    return total_log_likelihood

# Define the log-prior function
def log_prior(beta_vector):
    # Priors: N(0, 5) for beta_netflix, beta_prime, beta_ads
    # N(0, 1) for beta_price
    
    # Log-PDF for Netflix, Prime, Ads (indices 0, 1, 2)
    log_prior_binary = norm.logpdf(beta_vector[0], loc=0, scale=5) + \
                       norm.logpdf(beta_vector[1], loc=0, scale=5) + \
                       norm.logpdf(beta_vector[2], loc=0, scale=5)
    
    # Log-PDF for Price (index 3)
    log_prior_price = norm.logpdf(beta_vector[3], loc=0, scale=1)
    
    return log_prior_binary + log_prior_price

# Define the log-posterior function
def log_posterior(beta_vector, data):
    # Check for invalid beta_vector values (e.g., if they are NaN or Inf)
    if not np.all(np.isfinite(beta_vector)):
        return -np.inf # Return negative infinity if parameters are invalid

    # Calculate log-likelihood
    log_lik = mnl_log_likelihood(beta_vector, data)
    
    # If log_lik is -inf or NaN (e.g., due to probabilities becoming 0 or numerical issues)
    if not np.isfinite(log_lik):
        return -np.inf

    # Calculate log-prior
    log_p = log_prior(beta_vector)
    
    # Sum log-likelihood and log-prior to get log-posterior
    return log_lik + log_p

# --- Metropolis-Hastings MCMC Sampler ---

# Number of steps
num_steps = 11000
burn_in = 1000
retained_samples = 10000 # num_steps - burn_in

# Initial parameters (can start from MLEs or random values)
initial_betas_mcmc = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64) 

# Proposal distribution standard deviations
proposal_stds = np.array([np.sqrt(0.05), np.sqrt(0.05), np.sqrt(0.05), np.sqrt(0.005)], dtype=np.float64)

# Storage for samples
mcmc_samples = []
current_betas = initial_betas_mcmc.copy()

print("\n--- Starting Metropolis-Hastings MCMC Sampling ---")
print(f"Total steps: {num_steps}, Burn-in: {burn_in}, Retained samples: {retained_samples}")

for step in range(num_steps):
    # Propose new parameters
    proposed_betas = current_betas + np.random.normal(loc=0, scale=proposal_stds, size=len(current_betas))

    # Calculate log-posterior for current and proposed parameters
    log_post_current = log_posterior(current_betas, df_prep)
    log_post_proposed = log_posterior(proposed_betas, df_prep)

    # Calculate acceptance ratio in log-space
    log_alpha = log_post_proposed - log_post_current

    # Accept or reject the proposed parameters
    if log_alpha >= 0 or np.random.rand() < np.exp(log_alpha):
        current_betas = proposed_betas
    
    mcmc_samples.append(current_betas.copy()) # Store a copy of the current betas

    if (step + 1) % 1000 == 0:
        print(f"Step {step + 1}/{num_steps} completed. Current betas: {current_betas}")

print("\n--- MCMC Sampling Complete ---")

# Discard burn-in samples and retain the rest
posterior_samples_array = np.array(mcmc_samples[burn_in:])
posterior_df = pd.DataFrame(posterior_samples_array, columns=['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price'])

# --- Visualization: Trace Plot and Histogram ---

# Choose one parameter to plot (e.g., Beta_Netflix)
param_to_plot = 'Beta_Netflix'

plt.figure(figsize=(14, 6))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df[param_to_plot])
plt.title(f'Trace Plot for {param_to_plot}')
plt.xlabel('Sample Index')
plt.ylabel('Parameter Value')
plt.grid(True, linestyle='--', alpha=0.6)

# Histogram of posterior distribution
plt.subplot(1, 2, 2)
sns.histplot(posterior_df[param_to_plot], kde=True, bins=30)
plt.title(f'Posterior Distribution of {param_to_plot}')
plt.xlabel('Parameter Value')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

# --- Report Posterior Means, Standard Deviations, and 95% Credible Intervals ---

print("\n--- Bayesian Posterior Summary Statistics ---")
posterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).loc[['mean', 'std', '2.5%', '97.5%']]
print(posterior_summary)

# --- Comparison with Maximum Likelihood Approach ---
# To ensure MLE results are always available for comparison, we define them here.
# If you run the MLE section separately, these variables will be overwritten, which is fine.

print("\n--- Calculating MLE results for comparison ---")
initial_betas_mle = np.array([0.1, 0.1, -0.1, -0.05], dtype=np.float64)
# The mnl_log_likelihood function returns positive log-likelihood for MCMC,
# but minimize needs negative log-likelihood for its objective.
result_mle = minimize(lambda b, d: -mnl_log_likelihood(b, d), initial_betas_mle, args=(df_prep,), method='BFGS')
mle_betas = result_mle.x
cov_matrix_mle = result_mle.hess_inv
std_errors_mle = np.sqrt(np.diag(cov_matrix_mle))
z_value = 1.96
conf_intervals_mle = []
for i in range(len(mle_betas)):
    lower_bound = mle_betas[i] - z_value * std_errors_mle[i]
    upper_bound = mle_betas[i] + z_value * std_errors_mle[i]
    conf_intervals_mle.append((lower_bound, upper_bound))
param_names = ['Beta_Netflix', 'Beta_Prime', 'Beta_Ads', 'Beta_Price']
print("MLE results obtained for comparison.")


print("\n--- Comparison: Bayesian MCMC vs. Maximum Likelihood Estimation ---")
print("Parameter | Bayesian Mean | Bayesian Std | Bayesian 95% CI | MLE Estimate | MLE Std Err | MLE 95% CI")
print("-----------------------------------------------------------------------------------------------------------------")
for i, name in enumerate(param_names):
    bayesian_mean = posterior_summary.loc['mean', name]
    bayesian_std = posterior_summary.loc['std', name]
    bayesian_ci_lower = posterior_summary.loc['2.5%', name]
    bayesian_ci_upper = posterior_summary.loc['97.5%', name]
    
    mle_estimate = mle_betas[i]
    mle_std_err = std_errors_mle[i]
    mle_ci_lower = conf_intervals_mle[i][0]
    mle_ci_upper = conf_intervals_mle[i][1]

    print(f"{name:<9} | {bayesian_mean:13.4f} | {bayesian_std:12.4f} | [{bayesian_ci_lower:7.4f}, {bayesian_ci_upper:7.4f}] | {mle_estimate:12.4f} | {mle_std_err:11.4f} | [{mle_ci_lower:7.4f}, {mle_ci_upper:7.4f}]")

print("\n--- Interpretation ---")
print("In general, for sufficiently large sample sizes, Bayesian posterior means and standard deviations (and credible intervals) are expected to be similar to MLE estimates and their standard errors (and confidence intervals).")
print("Any notable differences could be due to:")
print("1. The choice of prior distributions (especially if the prior is very informative).")
print("2. Insufficient MCMC chain length or burn-in (check trace plots for convergence).")
print("3. Differences in how standard errors are calculated (asymptotic vs. posterior sample).")
print("4. Numerical stability issues in the likelihood function for extreme parameter values.")
print("The results from both methods should provide insights into the preference weights for different streaming service attributes.")
```

## 6. Discussion

Observations about Parameter Estimates
If we assume the data was not simulated (i.e., it represents real-world conjoint data) and we observe the parameter estimates:

General Observations on Estimates: You would typically examine the sign, magnitude, and statistical significance (or credibility) of each parameter.

Signs: Positive coefficients indicate a preference for that attribute level (e.g., higher utility), while negative coefficients suggest an aversion (lower utility).
Magnitude: The absolute magnitude of the coefficient reveals the strength of the preference or aversion. Larger absolute values imply a stronger impact on utility.
Significance/Credibility: If the 95% confidence interval (for MLE) or credible interval (for Bayesian) does not include zero, it suggests that the attribute level has a statistically significant or credible impact on utility, indicating the true effect is unlikely to be zero.
What does $\beta_{\text{Netflix}} > \beta_{\text{Prime}}$ mean?
This implies that, holding all other factors constant (such as price and ad exposure), consumers derive higher utility from Netflix compared to Prime Video. Since Hulu (or the brand_H dummy) was typically set as the reference category with a coefficient of zero, this observation also suggests that consumers prefer Netflix more than Prime Video, and both Netflix and Prime Video are preferred over Hulu (assuming both $\beta_{\text{Netflix}}$ and $\beta_{\text{Prime}}$ are positive and greater than zero). The difference in their magnitudes ($\beta_{\text{Netflix}} - \beta_{\text{Prime}}$) represents the incremental utility gain from choosing Netflix over Prime Video. For example, if $\beta_{\text{Netflix}} = 1.0$ and $\beta_{\text{Prime}} = 0.5$, opting for Netflix instead of Prime provides an additional 0.5 units of utility.

Does it make sense that $\beta_{\text{price}}$ is negative?
Yes, it makes perfect sense. Price represents a cost to the consumer. A negative coefficient for price indicates that as the price of an alternative increases, the utility derived from that alternative decreases. This aligns with fundamental economic principles of consumer behavior: all else being equal, consumers prefer lower prices. A larger absolute value for $\beta_{\text{price}}$ would imply that consumers are more sensitive to price changes.

Simulating Data and Estimating Parameters for a Multi-Level (Hierarchical) Model
A multi-level (also known as random-parameter or hierarchical) model is crucial for analyzing "real-world" conjoint data because it directly accounts for heterogeneity in preferences across individuals. Instead of assuming all respondents share the exact same underlying utility parameters (as in the simple MNL model you've implemented), it allows individual-specific parameters to vary according to a distribution (e.g., a normal distribution) across the population.

Here's a high-level overview of the changes needed to simulate data from and estimate parameters of such a model:

1. Simulating Data for a Multi-Level Model:
Individual-Specific Betas: Instead of a single "true" beta_vector for the entire population, you'd first define a population-level mean vector ($\mu_{\beta}$) and a covariance matrix ($\Sigma_{\beta}$) for the betas.
Sampling Individual Betas: For each simulated respondent ($i$), you would then draw their individual beta_vector_i from a multivariate normal distribution: $$\beta_i \sim \mathcal{N}(\mu_{\beta}, \Sigma_{\beta})$$
Generating Choices: For each respondent i and each choice task t, you'd use their specific beta_vector_i to calculate the utilities ($V_{ijt} = x_{jt}'\beta_i$) for all alternatives j in that task. Add extreme value (Gumbel) distributed error terms ($\varepsilon_{ijt}$) and determine the chosen alternative (choice_ijt) based on the highest total utility ($U_{ijt} = V_{ijt} + \varepsilon_{ijt}$), similar to your current simulation but personalized by individual betas.
Data Structure: The simulated data would maintain a similar observation-level structure (resp, task, attributes, choice), but the underlying beta_vector used to generate each choice would vary by resp.
2. Estimating Parameters for a Multi-Level Model:
Estimating these models is significantly more complex than standard MLE or basic MCMC, often requiring advanced Bayesian methods.

Model Formulation:

Likelihood: The likelihood for an individual's choices, given their specific betas, would remain similar to the MNL likelihood.
Priors: You'd need to define priors not only for the individual betas (which are now treated as random variables drawn from a distribution) but also for the hyper-parameters of that distributionâ€”specifically, the mean ($\mu_{\beta}$) and the covariance matrix ($\Sigma_{\beta}$) of the population-level beta distribution.
Hierarchical Structure: The model explicitly links individual-level parameters to population-level parameters, allowing for "borrowing strength" across individuals while acknowledging individual differences.
MCMC Sampler Changes:

Increased Parameter Space: The MCMC algorithm would need to sample not just the mean beta parameters but also the elements of the covariance matrix ($\Sigma_{\beta}$). For instance, with 4 betas, $\Sigma_{\beta}$ is a $4 \times 4$ matrix, introducing many more parameters (4 variances and 6 covariances) to estimate.
More Complex Proposal Distribution: The proposal distribution for sampling the individual beta_i's and the elements of $\Sigma_{\beta}$ would be more intricate. Specialized MCMC algorithms like Gibbs sampling or more advanced Metropolis-Hastings variants are commonly used, often drawing on techniques like slice sampling or Hamiltonian Monte Carlo (HMC) for greater efficiency, especially when dealing with the covariance matrix.
Data Partitioning: The log-likelihood calculation would involve summing log-likelihoods over each individual's choices, given their currently sampled individual-level betas.
Convergence Challenges: Multi-level MCMC models often require longer burn-in periods, more samples, and meticulous tuning of proposal distributions to ensure proper convergence due to the higher dimensionality and interdependencies of parameters.
Specialized Software: While possible to code from scratch (as you've done with basic MCMC), practitioners typically rely on robust probabilistic programming languages and libraries built for hierarchical modeling, such as:
PyMC (Python)
Stan (accessible via pystan in Python or rstan in R)
JAGS/BUGS (older, but still used)

In essence, building and estimating a multi-level conjoint model involves a significant leap in complexity, moving from a single set of population parameters to a distribution of individual parameters characterized by population-level means and covariances.







